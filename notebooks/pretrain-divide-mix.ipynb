{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "xSkgt1zf-raF",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
      "Collecting tqdm (from torch_geometric)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch_geometric)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m245.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m474.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Downloading multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: tqdm, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.4 aiosignal-1.3.2 frozenlist-1.6.0 multidict-6.4.4 propcache-0.3.1 torch_geometric-2.6.1 tqdm-4.67.1 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m226.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m283.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m433.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m440.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: pytz, tzdata, PySocks, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, gdown\n",
      "Successfully installed PySocks-1.7.1 contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.1 gdown-5.2.0 kiwisolver-1.4.8 matplotlib-3.10.3 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "5oR2D2Us-xSQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hackaton'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 81 (delta 4), reused 4 (delta 4), pack-reused 70 (from 1)\u001b[K\n",
      "Receiving objects: 100% (81/81), 105.83 MiB | 14.29 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n",
      "Updating files: 100% (38/38), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "PxBvwB0_6xI8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1oOxZgzm6GFEMqR9CE84syVkS3zDYm14G A\n",
      "Processing file 1KK7CrI83nsEBG4ft_BgBeB3FPfrtqfs6 test.json.gz\n",
      "Processing file 1HLDsT0NSttc9mI8obV4xsMdz0dXYsHg5 train.json.gz\n",
      "Retrieving folder 1fZMQBg3Zkd9k8D3ExvMHLPDZ-A4Uldz3 B\n",
      "Processing file 1nYi_Wj0c7FwMt1kNhenS5IOlfOAONUUO test.json.gz\n",
      "Processing file 14bnCHnrPdLQ9fmLbnoi3GKFrwfHy2JPN train.json.gz\n",
      "Retrieving folder 16I0ny5c6cuYncYTXNLro1i_bqUsJNeSc C\n",
      "Processing file 1HftG39UqLt5Zu7sYHcDl8GN6PqZV7i_M test.json.gz\n",
      "Processing file 1Q_QHBljWUS4ERBXbiZGQych-JKktRiMx train.json.gz\n",
      "Retrieving folder 1ToGahOt-r0Llmo6Bwq6WHnzq31Y6ixwm D\n",
      "Processing file 1yPgOhqJrmqP8eYfwCKgqdTz_blPZMzvj test.json.gz\n",
      "Processing file 1oZy8-zEj8heFGWAJ2loD6adqpdARVMlA train.json.gz\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1KK7CrI83nsEBG4ft_BgBeB3FPfrtqfs6\n",
      "From (redirected): https://drive.google.com/uc?id=1KK7CrI83nsEBG4ft_BgBeB3FPfrtqfs6&confirm=t&uuid=366a3259-9dcd-4a71-8c8f-f31385164f1f\n",
      "To: /workspace/hackaton/datasets/A/test.json.gz\n",
      "100%|██████████████████████████████████████| 92.4M/92.4M [00:01<00:00, 55.3MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1HLDsT0NSttc9mI8obV4xsMdz0dXYsHg5\n",
      "From (redirected): https://drive.google.com/uc?id=1HLDsT0NSttc9mI8obV4xsMdz0dXYsHg5&confirm=t&uuid=bfbcc555-a150-4efa-8904-32cc9f966e87\n",
      "To: /workspace/hackaton/datasets/A/train.json.gz\n",
      "100%|████████████████████████████████████████| 465M/465M [00:07<00:00, 64.2MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1nYi_Wj0c7FwMt1kNhenS5IOlfOAONUUO\n",
      "From (redirected): https://drive.google.com/uc?id=1nYi_Wj0c7FwMt1kNhenS5IOlfOAONUUO&confirm=t&uuid=4c8a8192-68d9-428d-be68-e1acc1e49f8c\n",
      "To: /workspace/hackaton/datasets/B/test.json.gz\n",
      "100%|██████████████████████████████████████| 63.0M/63.0M [00:01<00:00, 31.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14bnCHnrPdLQ9fmLbnoi3GKFrwfHy2JPN\n",
      "From (redirected): https://drive.google.com/uc?id=14bnCHnrPdLQ9fmLbnoi3GKFrwfHy2JPN&confirm=t&uuid=a7d78c5f-650d-4ee0-9ee6-a2e5dbb2335f\n",
      "To: /workspace/hackaton/datasets/B/train.json.gz\n",
      "100%|████████████████████████████████████████| 223M/223M [00:05<00:00, 44.3MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1HftG39UqLt5Zu7sYHcDl8GN6PqZV7i_M\n",
      "From (redirected): https://drive.google.com/uc?id=1HftG39UqLt5Zu7sYHcDl8GN6PqZV7i_M&confirm=t&uuid=8bbea293-0c91-43f6-a4ea-af84dfba5a66\n",
      "To: /workspace/hackaton/datasets/C/test.json.gz\n",
      "100%|██████████████████████████████████████| 60.5M/60.5M [00:02<00:00, 26.0MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Q_QHBljWUS4ERBXbiZGQych-JKktRiMx\n",
      "From (redirected): https://drive.google.com/uc?id=1Q_QHBljWUS4ERBXbiZGQych-JKktRiMx&confirm=t&uuid=bfc00ed6-6921-44d5-a18b-b67fd9860053\n",
      "To: /workspace/hackaton/datasets/C/train.json.gz\n",
      "100%|████████████████████████████████████████| 308M/308M [00:06<00:00, 44.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1yPgOhqJrmqP8eYfwCKgqdTz_blPZMzvj\n",
      "From (redirected): https://drive.google.com/uc?id=1yPgOhqJrmqP8eYfwCKgqdTz_blPZMzvj&confirm=t&uuid=0b1e96c8-87d4-4554-82c3-34289b6da9ba\n",
      "To: /workspace/hackaton/datasets/D/test.json.gz\n",
      "100%|██████████████████████████████████████| 94.0M/94.0M [00:02<00:00, 34.6MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1oZy8-zEj8heFGWAJ2loD6adqpdARVMlA\n",
      "From (redirected): https://drive.google.com/uc?id=1oZy8-zEj8heFGWAJ2loD6adqpdARVMlA&confirm=t&uuid=b0edcf5c-1d8c-4b6d-b38f-67e358d5a4ec\n",
      "To: /workspace/hackaton/datasets/D/train.json.gz\n",
      "100%|████████████████████████████████████████| 439M/439M [00:14<00:00, 30.4MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "# !gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n",
    "!gdown --folder https://drive.google.com/drive/folders/1vZVCPbux6brOnkfqil9CIj8a4iHlUwHV -O datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lAQuCuIoBbq5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "class GraphDatasetWithFeatures(Dataset):\n",
    "    def __init__(self, filename, transform=None, pre_transform=None):\n",
    "        self.raw = filename\n",
    "        self.num_graphs, self.graphs_dicts = self._count_graphs()\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return self.num_graphs\n",
    "\n",
    "    def get(self, idx):\n",
    "        return dictToGraphObjectWithFeatures(self.graphs_dicts[idx])\n",
    "\n",
    "    def _count_graphs(self):\n",
    "        with gzip.open(self.raw, \"rt\", encoding=\"utf-8\") as f:\n",
    "            graphs_dicts = json.load(f)\n",
    "            return len(graphs_dicts), graphs_dicts\n",
    "\n",
    "\n",
    "def dictToGraphObjectWithFeatures(graph_dict):\n",
    "    edge_index = torch.tensor(graph_dict[\"edge_index\"], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(graph_dict[\"edge_attr\"], dtype=torch.float) if graph_dict.get(\"edge_attr\") else None\n",
    "    num_nodes = graph_dict[\"num_nodes\"]\n",
    "    y = torch.tensor(graph_dict[\"y\"][0], dtype=torch.long) if graph_dict.get(\"y\") is not None else None\n",
    "\n",
    "    node_features = None\n",
    "    if edge_attr is not None:\n",
    "        node_feature_dim = edge_attr.shape[1]\n",
    "        node_features = torch.zeros((num_nodes, node_feature_dim), dtype=torch.float)\n",
    "\n",
    "        row, col = edge_index\n",
    "        node_features.index_add_(0, row, edge_attr)\n",
    "        node_features.index_add_(0, col, edge_attr)\n",
    "\n",
    "        deg = degree(edge_index[0], num_nodes=num_nodes, dtype=torch.float)\n",
    "        node_features = node_features / deg.unsqueeze(1)\n",
    "\n",
    "    graph_features = torch.tensor(graph_dict[\"graph_features\"], dtype=torch.float) if graph_dict.get(\"graph_features\") else None\n",
    "\n",
    "    return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_nodes, y=y, graph_x=graph_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, GCNConv, global_add_pool\n",
    "\n",
    "class GNN_node_Virtualnode_With_Features(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, drop_ratio=0.5, JK=\"last\", residual=False, gnn_type='gin', input_dim=None):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality (output dimension of the first layer, and hidden dim for subsequent layers)\n",
    "            input_dim (int): the actual dimension of the input node features (e.g., 7 for OGB-PPA derived features)\n",
    "        '''\n",
    "        super(GNN_node_Virtualnode_With_Features, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.residual = residual\n",
    "        self.emb_dim = emb_dim # This is the hidden dimension of the GNN layers\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        if input_dim is None:\n",
    "            raise ValueError(\"input_dim must be provided for GNN_node_Virtualnode (e.g., 7 for OGB-PPA).\")\n",
    "        self.node_encoder = torch.nn.Linear(input_dim, emb_dim)\n",
    "\n",
    "        ### set the initial virtual node embedding to 0.\n",
    "        self.virtualnode_embedding = torch.nn.Embedding(1, emb_dim)\n",
    "        torch.nn.init.constant_(self.virtualnode_embedding.weight.data, 0)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.mlp_virtualnode_list = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layer):\n",
    "            # PyG's GINConv and GCNConv expect `in_channels` and `out_channels`\n",
    "            # For GINConv, it takes an MLP, so we pass `emb_dim` as the input/output dim of the MLP.\n",
    "            # For GCNConv, it takes `in_channels` and `out_channels`.\n",
    "            if gnn_type == 'gin':\n",
    "                # The MLP passed to GINConv should map from emb_dim to emb_dim\n",
    "                self.convs.append(GINConv(torch.nn.Sequential(\n",
    "                    torch.nn.Linear(emb_dim, 2 * emb_dim),\n",
    "                    torch.nn.BatchNorm1d(2 * emb_dim),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(2 * emb_dim, emb_dim)\n",
    "                )))\n",
    "            elif gnn_type == 'gcn':\n",
    "                self.convs.append(GCNConv(emb_dim, emb_dim))\n",
    "            else:\n",
    "                raise ValueError('Undefined GNN type called {}'.format(gnn_type))\n",
    "\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "        for layer in range(num_layer - 1):\n",
    "            self.mlp_virtualnode_list.append(torch.nn.Sequential(\n",
    "                torch.nn.Linear(emb_dim, 2*emb_dim),\n",
    "                torch.nn.BatchNorm1d(2*emb_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(2*emb_dim, emb_dim),\n",
    "                torch.nn.BatchNorm1d(emb_dim),\n",
    "                torch.nn.ReLU() # ReLU here is consistent with the original\n",
    "            ))\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        x, edge_index, edge_attr, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.batch\n",
    "\n",
    "        if x is None:\n",
    "            raise ValueError(\"batched_data.x (node features) is None. Ensure your dataset correctly derives them.\")\n",
    "        h = self.node_encoder(x)\n",
    "        h_list = [h]\n",
    "\n",
    "        virtualnode_embedding = self.virtualnode_embedding(torch.zeros(batch[-1].item() + 1, dtype=torch.long, device=edge_index.device))\n",
    "\n",
    "        for layer in range(self.num_layer):\n",
    "            h_list[layer] = h_list[layer] + virtualnode_embedding[batch]\n",
    "\n",
    "            if self.convs[layer].__class__.__name__ == 'GINConv':\n",
    "                h = self.convs[layer](h_list[layer], edge_index) \n",
    "            elif self.convs[layer].__class__.__name__ == 'GCNConv':\n",
    "                h = self.convs[layer](h_list[layer], edge_index, edge_weight=edge_attr if edge_attr is not None else None) # GCNConv can take edge_weight\n",
    "            else:\n",
    "                h = self.convs[layer](h_list[layer], edge_index, edge_attr) \n",
    "\n",
    "            h = self.batch_norms[layer](h)\n",
    "\n",
    "            if layer == self.num_layer - 1:\n",
    "                h = F.dropout(h, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)\n",
    "\n",
    "            if self.residual:\n",
    "                h = h + h_list[layer] # Residual connection\n",
    "\n",
    "            h_list.append(h)\n",
    "\n",
    "            ### update the virtual nodes\n",
    "            if layer < self.num_layer - 1:\n",
    "                virtualnode_embedding_temp = global_add_pool(h_list[layer], batch) + virtualnode_embedding\n",
    "                ### transform virtual nodes using MLP\n",
    "\n",
    "                if self.residual:\n",
    "                    virtualnode_embedding = virtualnode_embedding + F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio, training=self.training)\n",
    "                else:\n",
    "                    virtualnode_embedding = F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio, training=self.training)\n",
    "\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"sum\":\n",
    "            node_representation = 0\n",
    "            # Iterate over all h_list (including initial embedding)\n",
    "            for h_rep in h_list:\n",
    "                node_representation += h_rep\n",
    "        else:\n",
    "            raise ValueError(\"Undefined JK type: {}\".format(self.JK))\n",
    "\n",
    "        return node_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "from src.conv import GNN_node, GNN_node_Virtualnode\n",
    "\n",
    "class GNN_With_Features(torch.nn.Module):\n",
    "    def __init__(self, num_class, num_layer=5, emb_dim=None, # <--- emb_dim can be dynamically set\n",
    "                 gnn_type='gin', virtual_node=True, residual=False,\n",
    "                 drop_ratio=0.5, JK=\"last\", graph_pooling=\"mean\",\n",
    "                 num_graph_features=0):\n",
    "        super(GNN_With_Features, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "        self.num_graph_features = num_graph_features\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        # If emb_dim is not provided, we will determine it from the first batch\n",
    "        # This requires `self.gnn_node` to handle None initially or be initialized later\n",
    "        # A safer approach is to pass the expected input dim to GNN_node\n",
    "        # For OGB-PPA, the input node feature dimension is 7 (from edge_attr)\n",
    "        self.input_node_feat_dim = 7 # <--- Explicitly set for OGB-PPA derived features\n",
    "        self.emb_dim = emb_dim if emb_dim is not None else self.input_node_feat_dim # Use derived dim if not specified\n",
    "\n",
    "        # GNN_node now needs to know the input dimension (self.input_node_feat_dim)\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode_With_Features(\n",
    "                num_layer,\n",
    "                self.emb_dim,\n",
    "                JK=JK,\n",
    "                drop_ratio=drop_ratio,\n",
    "                residual=residual,\n",
    "                gnn_type=gnn_type,\n",
    "                input_dim=self.input_node_feat_dim) # <--- Pass input_dim\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, self.emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual, gnn_type=gnn_type,\n",
    "                                    input_dim=self.input_node_feat_dim) # <--- Pass input_dim\n",
    "\n",
    "        ### Pooling function\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            # Gate network input dimension needs to match emb_dim\n",
    "            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(self.emb_dim, 2*self.emb_dim), torch.nn.BatchNorm1d(2*self.emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*self.emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(self.emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        # Final linear layer\n",
    "        if graph_pooling == \"set2set\":\n",
    "            pooled_dim = 2 * self.emb_dim\n",
    "        else:\n",
    "            pooled_dim = self.emb_dim\n",
    "\n",
    "        self.graph_pred_linear = torch.nn.Linear(pooled_dim + self.num_graph_features, self.num_class)\n",
    "\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # The GNN_node module expects batched_data.x for node features\n",
    "        # Your dictToGraphObject now correctly sets batched_data.x\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "\n",
    "        if self.num_graph_features > 0:\n",
    "            if not hasattr(batched_data, 'graph_x') or batched_data.graph_x is None:\n",
    "                raise ValueError(\"num_graph_features is set > 0, but 'graph_x' attribute not found in batched_data.\")\n",
    "            if batched_data.graph_x.shape[0] != h_graph.shape[0]:\n",
    "                raise ValueError(f\"Batch size of graph_x ({batched_data.graph_x.shape[0]}) does not match pooled graph embeddings ({h_graph.shape[0]}).\")\n",
    "\n",
    "            h_graph = torch.cat([h_graph, batched_data.graph_x], dim=-1)\n",
    "\n",
    "        return self.graph_pred_linear(h_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GeneralizedCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\n",
    "    2018 - Zhang et al. - https://arxiv.org/pdf/1805.07836.pdf\n",
    "    \n",
    "    Args:\n",
    "        q (float): Robustness parameter. q=0 gives standard cross-entropy, q=1 gives MAE.\n",
    "                  Recommended range: [0.1, 0.7] for noise robustness.\n",
    "        reduction (str): Specifies the reduction to apply to the output.\n",
    "        eps (float): Small value to prevent numerical instability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, q=0.3, reduction='mean', eps=1e-7):\n",
    "        super(GeneralizedCrossEntropyLoss, self).__init__()\n",
    "        self.q = q\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Tensor of shape (N, C) containing raw logits\n",
    "            targets: Tensor of shape (N,) containing class indices\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        y_pred = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert targets to one-hot encoding\n",
    "        y_true = F.one_hot(targets, num_classes=logits.size(-1)).float()\n",
    "        \n",
    "        # Compute GCE loss with clamping for numerical stability\n",
    "        # Clamp predictions to avoid numerical issues\n",
    "        y_pred = torch.clamp(y_pred, min=self.eps, max=1.0 - self.eps)\n",
    "        \n",
    "        # Compute the sum of y_true * y_pred along class dimension\n",
    "        pred_sum = torch.sum(y_true * y_pred, dim=-1)\n",
    "        \n",
    "        # Clamp again before taking the power to ensure stability\n",
    "        pred_sum = torch.clamp(pred_sum, min=self.eps)\n",
    "        \n",
    "        # GCE loss: (1 - pred_sum^q) / q\n",
    "        if self.q == 0:\n",
    "            # Special case: standard cross-entropy (limit as q->0)\n",
    "            loss = -torch.log(pred_sum)\n",
    "        else:\n",
    "            intermed = torch.pow(pred_sum, self.q)\n",
    "            loss = (1 - intermed) / self.q\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        else:  # 'none'\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x1OnGq_nCmTr"
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, criterion=None, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                if criterion is not None:\n",
    "                    total_loss += criterion(output, data.y).item()\n",
    "\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    \n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader), accuracy\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    # script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "script_dir = os.getcwd()\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_name,\n",
    "            noise_ratio=0.2,\n",
    "            num_layer=5,\n",
    "            emb_dim=256,\n",
    "            drop_ratio=0.2,\n",
    "            batch_size=32,\n",
    "    ):\n",
    "        self.train_path = f\"datasets/{dataset_name}/train.json.gz\"\n",
    "        self.test_path = f\"datasets/{dataset_name}/test.json.gz\"\n",
    "\n",
    "        self.noise_ratio = noise_ratio\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.emb_dim = emb_dim\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.batch_size = batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = GraphDatasetWithFeatures(args.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())  # Console output as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path relative to the script's directory\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(0.2 * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(12)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "from torch_geometric.nn.inits import uniform\n",
    "import numpy as np\n",
    "\n",
    "# 1. CONTRASTIVE LEARNING APPROACH\n",
    "class GNNContrastive(torch.nn.Module):\n",
    "    \"\"\"GNN with contrastive learning for representation learning\"\"\"\n",
    "    def __init__(self, num_class, num_layer=5, emb_dim=None, \n",
    "                 gnn_type='gin', virtual_node=True, residual=False,\n",
    "                 drop_ratio=0.5, JK=\"last\", graph_pooling=\"mean\",\n",
    "                 num_graph_features=0, projection_dim=128):\n",
    "        super(GNNContrastive, self).__init__()\n",
    "        \n",
    "        # Copy your existing GNN initialization\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "        self.num_graph_features = num_graph_features\n",
    "        self.input_node_feat_dim = 7\n",
    "        self.emb_dim = emb_dim if emb_dim is not None else self.input_node_feat_dim\n",
    "\n",
    "        # Your existing GNN_node initialization\n",
    "        from src.conv import GNN_node\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode_With_Features(\n",
    "                num_layer,\n",
    "                self.emb_dim,\n",
    "                JK=JK,\n",
    "                drop_ratio=drop_ratio,\n",
    "                residual=residual,\n",
    "                gnn_type=gnn_type,\n",
    "                input_dim=self.input_node_feat_dim)\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, self.emb_dim, JK=JK, \n",
    "                                   drop_ratio=drop_ratio, residual=residual, \n",
    "                                   gnn_type=gnn_type, input_dim=self.input_node_feat_dim)\n",
    "\n",
    "        # Pooling (same as your code)\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            self.pool = GlobalAttention(gate_nn=torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.emb_dim, 2*self.emb_dim), \n",
    "                torch.nn.BatchNorm1d(2*self.emb_dim), \n",
    "                torch.nn.ReLU(), \n",
    "                torch.nn.Linear(2*self.emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(self.emb_dim, processing_steps=2)\n",
    "\n",
    "        # Calculate pooled dimension\n",
    "        if graph_pooling == \"set2set\":\n",
    "            pooled_dim = 2 * self.emb_dim\n",
    "        else:\n",
    "            pooled_dim = self.emb_dim\n",
    "            \n",
    "        self.pooled_dim = pooled_dim + self.num_graph_features\n",
    "\n",
    "        # Projection head for contrastive learning\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.pooled_dim, self.pooled_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.pooled_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(self.pooled_dim, self.num_class)\n",
    "        \n",
    "    def forward(self, batched_data, return_projection=False):\n",
    "        \"\"\"Forward pass with option to return projection for contrastive learning\"\"\"\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "\n",
    "        if self.num_graph_features > 0:\n",
    "            if not hasattr(batched_data, 'graph_x') or batched_data.graph_x is None:\n",
    "                raise ValueError(\"num_graph_features is set > 0, but 'graph_x' attribute not found.\")\n",
    "            h_graph = torch.cat([h_graph, batched_data.graph_x], dim=-1)\n",
    "\n",
    "        if return_projection:\n",
    "            projection = F.normalize(self.projection_head(h_graph), dim=1)\n",
    "            return h_graph, projection\n",
    "        \n",
    "        return self.classifier(h_graph)\n",
    "    \n",
    "    def get_representations(self, batched_data):\n",
    "        \"\"\"Get graph representations without classification\"\"\"\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "        \n",
    "        if self.num_graph_features > 0:\n",
    "            if hasattr(batched_data, 'graph_x') and batched_data.graph_x is not None:\n",
    "                h_graph = torch.cat([h_graph, batched_data.graph_x], dim=-1)\n",
    "        \n",
    "        return h_graph\n",
    "\n",
    "\n",
    "# 2. NOISE-ROBUST LOSS FUNCTIONS\n",
    "class SymmetricCrossEntropy(nn.Module):\n",
    "    \"\"\"Symmetric Cross-Entropy Loss for noisy labels\"\"\"\n",
    "    def __init__(self, alpha=0.1, beta=1.0, num_classes=6):\n",
    "        super(SymmetricCrossEntropy, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        # Standard cross-entropy\n",
    "        ce = F.cross_entropy(pred, labels, reduction='none')\n",
    "        \n",
    "        # Reverse cross-entropy\n",
    "        pred_softmax = F.softmax(pred, dim=1)\n",
    "        pred_softmax = torch.clamp(pred_softmax, min=1e-7, max=1.0)\n",
    "        \n",
    "        label_one_hot = F.one_hot(labels, self.num_classes).float()\n",
    "        rce = -1 * torch.sum(pred_softmax * torch.log(label_one_hot + 1e-7), dim=1)\n",
    "        \n",
    "        # Combine both losses\n",
    "        loss = self.alpha * ce + self.beta * rce\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class GeneralizedCrossEntropy(nn.Module):\n",
    "    \"\"\"Generalized Cross-Entropy Loss\"\"\"\n",
    "    def __init__(self, q=0.7):\n",
    "        super(GeneralizedCrossEntropy, self).__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred_softmax = F.softmax(pred, dim=1)\n",
    "        pred_softmax = torch.clamp(pred_softmax, min=1e-7, max=1.0)\n",
    "        \n",
    "        label_one_hot = F.one_hot(labels, pred.shape[1]).float()\n",
    "        loss = (1 - torch.sum(label_one_hot * pred_softmax**self.q, dim=1)) / self.q\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# 3. CONTRASTIVE LEARNING TRAINING\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"InfoNCE Loss for contrastive learning\"\"\"\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        \n",
    "        # Create positive pairs mask (same class)\n",
    "        if labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            mask = torch.eq(labels, labels.T).float()\n",
    "            # Remove diagonal (self-similarity)\n",
    "            mask = mask.fill_diagonal_(0)\n",
    "        else:\n",
    "            # If no labels, create augmentation-based positive pairs\n",
    "            # This requires data augmentation strategy\n",
    "            mask = torch.eye(batch_size).to(features.device)\n",
    "            mask = torch.cat([mask[batch_size//2:], mask[:batch_size//2]], dim=0)\n",
    "        \n",
    "        # Apply contrastive loss\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        sum_exp_sim = exp_sim.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        positive_sim = (exp_sim * mask).sum(dim=1)\n",
    "        loss = -torch.log(positive_sim / (sum_exp_sim.squeeze() + 1e-8))\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# 4. TRAINING PIPELINE WITH REPRESENTATION LEARNING\n",
    "class NoiseRobustTrainer:\n",
    "    def __init__(self, model, device='cuda', warmup_epochs=10):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        \n",
    "        # Different loss functions\n",
    "        self.contrastive_loss = ContrastiveLoss()\n",
    "        self.sce_loss = SymmetricCrossEntropy()\n",
    "        self.gce_loss = GeneralizedCrossEntropy()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "    def pretrain_representations(self, dataloader, epochs=50):\n",
    "        \"\"\"Pretrain with contrastive learning\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                \n",
    "                # Get representations and projections\n",
    "                representations, projections = self.model(batch, return_projection=True)\n",
    "                \n",
    "                # Contrastive loss (unsupervised)\n",
    "                loss = self.contrastive_loss(projections)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Pretrain Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "    \n",
    "    def train_with_noise_robustness(self, dataloader, epochs=100):\n",
    "        \"\"\"Train with noise-robust losses\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                labels = batch.y\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = self.model(batch)\n",
    "                \n",
    "                # Choose loss based on epoch\n",
    "                if epoch < self.warmup_epochs:\n",
    "                    # Use symmetric cross-entropy for warmup\n",
    "                    loss = self.sce_loss(logits, labels)\n",
    "                else:\n",
    "                    # Use generalized cross-entropy\n",
    "                    loss = self.gce_loss(logits, labels)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            print(f'Train Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "\n",
    "# 5. SAMPLE SELECTION STRATEGY\n",
    "class SampleSelector:\n",
    "    def __init__(self, model, threshold_percentile=30):\n",
    "        self.model = model\n",
    "        self.threshold_percentile = threshold_percentile\n",
    "    \n",
    "    def select_clean_samples(self, dataloader):\n",
    "        \"\"\"Select samples with low loss (likely clean)\"\"\"\n",
    "        self.model.eval()\n",
    "        all_losses = []\n",
    "        all_indices = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                batch = batch.to(next(self.model.parameters()).device)\n",
    "                logits = self.model(batch)\n",
    "                losses = F.cross_entropy(logits, batch.y, reduction='none')\n",
    "                \n",
    "                all_losses.extend(losses.cpu().numpy())\n",
    "                # You'll need to track original indices\n",
    "                all_indices.extend(range(batch_idx * batch.batch_size, \n",
    "                                       (batch_idx + 1) * batch.batch_size))\n",
    "        \n",
    "        # Select samples with losses below threshold\n",
    "        threshold = np.percentile(all_losses, self.threshold_percentile)\n",
    "        clean_indices = [idx for idx, loss in zip(all_indices, all_losses) \n",
    "                        if loss < threshold]\n",
    "        \n",
    "        return clean_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNContrastive(\n",
    "    gnn_type='gin',\n",
    "    num_class=6,\n",
    "    num_layer=args.num_layer,\n",
    "    emb_dim=args.emb_dim,\n",
    "    drop_ratio=args.drop_ratio,\n",
    "    virtual_node=True,\n",
    "    residual=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NoiseRobustTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining representations...\n",
      "Pretrain Epoch 0, Loss: 3.4731\n",
      "Pretrain Epoch 10, Loss: 3.4658\n",
      "Pretrain Epoch 20, Loss: 3.4658\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Pretrain representations\n",
    "print(\"Pretraining representations...\")\n",
    "trainer.pretrain_representations(train_loader, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with noise-robust losses...\n",
      "Train Epoch 0, Loss: 13.3444\n",
      "Train Epoch 1, Loss: 12.2009\n",
      "Train Epoch 2, Loss: 11.6601\n",
      "Train Epoch 3, Loss: 12.1428\n",
      "Train Epoch 4, Loss: 11.5368\n",
      "Train Epoch 5, Loss: 11.2384\n",
      "Train Epoch 6, Loss: 11.1259\n",
      "Train Epoch 7, Loss: 11.5249\n",
      "Train Epoch 8, Loss: 11.3281\n",
      "Train Epoch 9, Loss: 10.9869\n",
      "Train Epoch 10, Loss: 0.8713\n",
      "Train Epoch 11, Loss: 0.8716\n",
      "Train Epoch 12, Loss: 0.8399\n",
      "Train Epoch 13, Loss: 0.8343\n",
      "Train Epoch 14, Loss: 0.8395\n",
      "Train Epoch 15, Loss: 0.8196\n",
      "Train Epoch 16, Loss: 0.8107\n",
      "Train Epoch 17, Loss: 0.8047\n",
      "Train Epoch 18, Loss: 0.8056\n",
      "Train Epoch 19, Loss: 0.7991\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train with noise-robust losses\n",
    "print(\"Training with noise-robust losses...\")\n",
    "trainer.train_with_noise_robustness(clean_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with noise-robust losses...\n",
      "Train Epoch 0, Loss: 10.6582\n",
      "Train Epoch 1, Loss: 10.4948\n",
      "Train Epoch 2, Loss: 10.3348\n",
      "Train Epoch 3, Loss: 10.0820\n",
      "Train Epoch 4, Loss: 9.9448\n",
      "Train Epoch 5, Loss: 9.8221\n",
      "Train Epoch 6, Loss: 9.7185\n",
      "Train Epoch 7, Loss: 9.6640\n",
      "Train Epoch 8, Loss: 9.6264\n",
      "Train Epoch 9, Loss: 9.5566\n",
      "Train Epoch 10, Loss: 0.7630\n",
      "Train Epoch 11, Loss: 0.7422\n",
      "Train Epoch 12, Loss: 0.7421\n",
      "Train Epoch 13, Loss: 0.7397\n",
      "Train Epoch 14, Loss: 0.7339\n",
      "Train Epoch 15, Loss: 0.7339\n",
      "Train Epoch 16, Loss: 0.7345\n",
      "Train Epoch 17, Loss: 0.7282\n",
      "Train Epoch 18, Loss: 0.7264\n",
      "Train Epoch 19, Loss: 0.7229\n",
      "Train Epoch 20, Loss: 0.7245\n",
      "Train Epoch 21, Loss: 0.7196\n",
      "Train Epoch 22, Loss: 0.7221\n",
      "Train Epoch 23, Loss: 0.7235\n",
      "Train Epoch 24, Loss: 0.7199\n",
      "Train Epoch 25, Loss: 0.7175\n",
      "Train Epoch 26, Loss: 0.7132\n",
      "Train Epoch 27, Loss: 0.7179\n",
      "Train Epoch 28, Loss: 0.7194\n",
      "Train Epoch 29, Loss: 0.7120\n",
      "Train Epoch 30, Loss: 0.7191\n",
      "Train Epoch 31, Loss: 0.7156\n",
      "Train Epoch 32, Loss: 0.7129\n",
      "Train Epoch 33, Loss: 0.7160\n",
      "Train Epoch 34, Loss: 0.7115\n",
      "Train Epoch 35, Loss: 0.7150\n",
      "Train Epoch 36, Loss: 0.7119\n",
      "Train Epoch 37, Loss: 0.7079\n",
      "Train Epoch 38, Loss: 0.7027\n",
      "Train Epoch 39, Loss: 0.7125\n",
      "Train Epoch 40, Loss: 0.7153\n",
      "Train Epoch 41, Loss: 0.7172\n",
      "Train Epoch 42, Loss: 0.7135\n",
      "Train Epoch 43, Loss: 0.7090\n",
      "Train Epoch 44, Loss: 0.7162\n",
      "Train Epoch 45, Loss: 0.7056\n",
      "Train Epoch 46, Loss: 0.7130\n",
      "Train Epoch 47, Loss: 0.7100\n",
      "Train Epoch 48, Loss: 0.7110\n",
      "Train Epoch 49, Loss: 0.7118\n",
      "Train Epoch 50, Loss: 0.7019\n",
      "Train Epoch 51, Loss: 0.7031\n",
      "Train Epoch 52, Loss: 0.7184\n",
      "Train Epoch 53, Loss: 0.7027\n",
      "Train Epoch 54, Loss: 0.7074\n",
      "Train Epoch 55, Loss: 0.7064\n",
      "Train Epoch 56, Loss: 0.7027\n",
      "Train Epoch 57, Loss: 0.7103\n",
      "Train Epoch 58, Loss: 0.7057\n",
      "Train Epoch 59, Loss: 0.6958\n",
      "Train Epoch 60, Loss: 0.6909\n",
      "Train Epoch 61, Loss: 0.7004\n",
      "Train Epoch 62, Loss: 0.6994\n",
      "Train Epoch 63, Loss: 0.7017\n",
      "Train Epoch 64, Loss: 0.6962\n",
      "Train Epoch 65, Loss: 0.6893\n",
      "Train Epoch 66, Loss: 0.6951\n",
      "Train Epoch 67, Loss: 0.6913\n",
      "Train Epoch 68, Loss: 0.6880\n",
      "Train Epoch 69, Loss: 0.6930\n",
      "Train Epoch 70, Loss: 0.7021\n",
      "Train Epoch 71, Loss: 0.6969\n",
      "Train Epoch 72, Loss: 0.6847\n",
      "Train Epoch 73, Loss: 0.6900\n",
      "Train Epoch 74, Loss: 0.6940\n",
      "Train Epoch 75, Loss: 0.6937\n",
      "Train Epoch 76, Loss: 0.6920\n",
      "Train Epoch 77, Loss: 0.6833\n",
      "Train Epoch 78, Loss: 0.6809\n",
      "Train Epoch 79, Loss: 0.6882\n",
      "Train Epoch 80, Loss: 0.6816\n",
      "Train Epoch 81, Loss: 0.6837\n",
      "Train Epoch 82, Loss: 0.6767\n",
      "Train Epoch 83, Loss: 0.6844\n",
      "Train Epoch 84, Loss: 0.6833\n",
      "Train Epoch 85, Loss: 0.6691\n",
      "Train Epoch 86, Loss: 0.6754\n",
      "Train Epoch 87, Loss: 0.6706\n",
      "Train Epoch 88, Loss: 0.6869\n",
      "Train Epoch 89, Loss: 0.6801\n",
      "Train Epoch 90, Loss: 0.6681\n",
      "Train Epoch 91, Loss: 0.6760\n",
      "Train Epoch 92, Loss: 0.6711\n",
      "Train Epoch 93, Loss: 0.6744\n",
      "Train Epoch 94, Loss: 0.6683\n",
      "Train Epoch 95, Loss: 0.6772\n",
      "Train Epoch 96, Loss: 0.6595\n",
      "Train Epoch 97, Loss: 0.6657\n",
      "Train Epoch 98, Loss: 0.6647\n",
      "Train Epoch 99, Loss: 0.6650\n"
     ]
    }
   ],
   "source": [
    "print(\"Training with noise-robust losses...\")\n",
    "trainer.train_with_noise_robustness(train_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Optional - Sample selection for refinement\n",
    "selector = SampleSelector(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1315 clean samples\n"
     ]
    }
   ],
   "source": [
    "clean_indices_train = selector.select_clean_samples(train_loader)\n",
    "print(f\"Selected {len(clean_indices_train)} clean samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 336 clean samples\n"
     ]
    }
   ],
   "source": [
    "clean_indices_val = selector.select_clean_samples(val_loader)\n",
    "print(f\"Selected {len(clean_indices_val)} clean samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def create_subset_dataloader(dataset, clean_indices, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a new dataloader with only the selected clean samples\n",
    "    \n",
    "    Args:\n",
    "        dataset: Your GraphDatasetWithFeatures instance\n",
    "        clean_indices: List of indices for clean samples\n",
    "        batch_size: Batch size for the dataloader\n",
    "        shuffle: Whether to shuffle the data\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader with subset of clean samples\n",
    "    \"\"\"\n",
    "    subset = Subset(dataset, clean_indices)\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = create_subset_dataloader(train_dataset, clean_indices_train, batch_size=args.batch_size, shuffle=True)\n",
    "clean_val = create_subset_dataloader(val_dataset, clean_indices_val, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/workspace/hackaton/checkpoints/model_B_best.pth_pretrained.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m376.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m369.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class DivideMixGraphTrainer:\n",
    "    \"\"\"\n",
    "    DivideMix-style training adapted for graph data with high noise rates\n",
    "    Enhanced with validation and monitoring capabilities\n",
    "    \"\"\"\n",
    "    def __init__(self, model1, model2, dataset, val_dataset=None, device='cuda', warmup_epochs=1):\n",
    "        self.model1 = model1.to(device)\n",
    "        self.model2 = model2.to(device)\n",
    "        self.dataset = dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.device = device\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        \n",
    "        self.optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "        self.optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "        \n",
    "        # Track clean/noisy split for each model\n",
    "        self.clean_indices1 = []\n",
    "        self.clean_indices2 = []\n",
    "        \n",
    "        # Validation tracking\n",
    "        self.train_history = {'loss': [], 'accuracy': []}\n",
    "        self.val_history = {'loss': [], 'accuracy': [], 'f1': []}\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_models = {'model1': None, 'model2': None}\n",
    "        \n",
    "        # Early stopping\n",
    "        self.patience = 20\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def evaluate_model(self, model, dataloader, return_predictions=False):\n",
    "        \"\"\"Evaluate model on given dataloader\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                logits = model(batch)\n",
    "                loss = F.cross_entropy(logits, batch.y)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch.y.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        if return_predictions:\n",
    "            return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "        return avg_loss, accuracy, f1\n",
    "    \n",
    "    def validate_models(self, val_dataloader, epoch):\n",
    "        \"\"\"Validate both models and track best performance\"\"\"\n",
    "        if val_dataloader is None:\n",
    "            return\n",
    "        \n",
    "        # Evaluate both models\n",
    "        val_loss1, val_acc1, val_f1_1 = self.evaluate_model(self.model1, val_dataloader)\n",
    "        val_loss2, val_acc2, val_f1_2 = self.evaluate_model(self.model2, val_dataloader)\n",
    "        \n",
    "        # Use ensemble average for validation metrics\n",
    "        avg_val_loss = (val_loss1 + val_loss2) / 2\n",
    "        avg_val_acc = (val_acc1 + val_acc2) / 2\n",
    "        avg_val_f1 = (val_f1_1 + val_f1_2) / 2\n",
    "        \n",
    "        # Track validation history\n",
    "        self.val_history['loss'].append(avg_val_loss)\n",
    "        self.val_history['accuracy'].append(avg_val_acc)\n",
    "        self.val_history['f1'].append(avg_val_f1)\n",
    "        \n",
    "        # Save best models\n",
    "        if avg_val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = avg_val_acc\n",
    "            self.best_models['model1'] = {\n",
    "                'state_dict': self.model1.state_dict().copy(),\n",
    "                'optimizer': self.optimizer1.state_dict().copy()\n",
    "            }\n",
    "            self.best_models['model2'] = {\n",
    "                'state_dict': self.model2.state_dict().copy(),\n",
    "                'optimizer': self.optimizer2.state_dict().copy()\n",
    "            }\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch} - Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Acc: {avg_val_acc:.4f}, Val F1: {avg_val_f1:.4f}\")\n",
    "        \n",
    "        return avg_val_acc\n",
    "    \n",
    "    def should_early_stop(self):\n",
    "        \"\"\"Check if early stopping criteria is met\"\"\"\n",
    "        return self.patience_counter >= self.patience\n",
    "    \n",
    "    def load_best_models(self):\n",
    "        \"\"\"Load the best performing models\"\"\"\n",
    "        if self.best_models['model1'] is not None:\n",
    "            self.model1.load_state_dict(self.best_models['model1']['state_dict'])\n",
    "            self.optimizer1.load_state_dict(self.best_models['model1']['optimizer'])\n",
    "            \n",
    "        if self.best_models['model2'] is not None:\n",
    "            self.model2.load_state_dict(self.best_models['model2']['state_dict'])\n",
    "            self.optimizer2.load_state_dict(self.best_models['model2']['optimizer'])\n",
    "    \n",
    "    def warmup_training(self, dataloader, val_dataloader=None, epochs=10):\n",
    "        \"\"\"Warmup phase with symmetric cross-entropy and validation\"\"\"\n",
    "        print(\"Warmup training phase...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train both models\n",
    "            loss1 = self._train_epoch_warmup(self.model1, self.optimizer1, dataloader)\n",
    "            loss2 = self._train_epoch_warmup(self.model2, self.optimizer2, dataloader)\n",
    "            \n",
    "            avg_loss = (loss1 + loss2) / 2\n",
    "            self.train_history['loss'].append(avg_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if val_dataloader is not None:\n",
    "                self.validate_models(val_dataloader, epoch)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Warmup epoch {epoch}/{epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def _train_epoch_warmup(self, model, optimizer, dataloader):\n",
    "        \"\"\"Single warmup epoch with symmetric cross-entropy\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch)\n",
    "            \n",
    "            # Symmetric Cross-Entropy Loss\n",
    "            ce_loss = F.cross_entropy(logits, batch.y, reduction='none')\n",
    "            \n",
    "            # Reverse cross-entropy\n",
    "            pred_softmax = F.softmax(logits, dim=1)\n",
    "            pred_softmax = torch.clamp(pred_softmax, min=1e-7, max=1.0)\n",
    "            label_one_hot = F.one_hot(batch.y, logits.shape[1]).float()\n",
    "            rce_loss = -torch.sum(pred_softmax * torch.log(label_one_hot + 1e-7), dim=1)\n",
    "            \n",
    "            loss = (0.1 * ce_loss + 1.0 * rce_loss).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def divide_samples(self, model, dataloader, clean_ratio=0.5):\n",
    "        \"\"\"Divide samples into clean and noisy based on small loss\"\"\"\n",
    "        model.eval()\n",
    "        all_losses = []\n",
    "        all_indices = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            global_idx = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                logits = model(batch)\n",
    "                losses = F.cross_entropy(logits, batch.y, reduction='none')\n",
    "                \n",
    "                for i, loss in enumerate(losses):\n",
    "                    all_losses.append(loss.item())\n",
    "                    all_indices.append(global_idx + i)\n",
    "                \n",
    "                global_idx += len(batch.y)\n",
    "        \n",
    "        # Sort by loss and take clean_ratio as clean samples\n",
    "        sorted_indices = sorted(zip(all_indices, all_losses), key=lambda x: x[1])\n",
    "        num_clean = int(len(sorted_indices) * clean_ratio)\n",
    "        \n",
    "        clean_indices = [idx for idx, _ in sorted_indices[:num_clean]]\n",
    "        noisy_indices = [idx for idx, _ in sorted_indices[num_clean:]]\n",
    "        \n",
    "        return clean_indices, noisy_indices\n",
    "    \n",
    "    def mixup_data(self, batch1, batch2, alpha=0.75):\n",
    "        \"\"\"\n",
    "        Alternative MixUp for graph data - uses label mixing only\n",
    "        This avoids the complexity of mixing variable-sized graph structures\n",
    "        \"\"\"\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "        \n",
    "        # Determine batch size for mixing\n",
    "        batch_size = min(len(batch1.y), len(batch2.y))\n",
    "        \n",
    "        # Sample indices from both batches\n",
    "        indices1 = torch.randperm(len(batch1.y))[:batch_size]\n",
    "        indices2 = torch.randperm(len(batch2.y))[:batch_size]\n",
    "        \n",
    "        # Create mixed labels using selected samples\n",
    "        y1_selected = batch1.y[indices1]\n",
    "        y2_selected = batch2.y[indices2]\n",
    "        \n",
    "        y1_one_hot = F.one_hot(y1_selected, num_classes=self.model1.num_class).float()\n",
    "        y2_one_hot = F.one_hot(y2_selected, num_classes=self.model1.num_class).float()\n",
    "        mixed_targets = lam * y1_one_hot + (1 - lam) * y2_one_hot\n",
    "        \n",
    "        # Return the first batch for forward pass, but with mixed supervision\n",
    "        return batch1, mixed_targets, lam, indices1\n",
    "    \n",
    "    def graph_feature_mixup(self, batch1, batch2, alpha=0.75):\n",
    "        \"\"\"\n",
    "        Advanced MixUp that handles graph structure mixing\n",
    "        Only use this if your graphs have similar structures\n",
    "        \"\"\"\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "        \n",
    "        # Check if we can mix at node level (similar graph sizes)\n",
    "        if abs(batch1.x.size(0) - batch2.x.size(0)) / max(batch1.x.size(0), batch2.x.size(0)) < 0.1:\n",
    "            # Graphs are similar size, can mix node features\n",
    "            min_nodes = min(batch1.x.size(0), batch2.x.size(0))\n",
    "            \n",
    "            # Truncate to same size\n",
    "            mixed_x = lam * batch1.x[:min_nodes] + (1 - lam) * batch2.x[:min_nodes]\n",
    "            \n",
    "            # Create new batch\n",
    "            mixed_batch = batch1.clone()\n",
    "            mixed_batch.x = mixed_x[:min_nodes]\n",
    "            \n",
    "            # Adjust other batch attributes if needed\n",
    "            if hasattr(mixed_batch, 'batch'):\n",
    "                # Truncate batch indices\n",
    "                valid_batch_mask = mixed_batch.batch < min_nodes\n",
    "                mixed_batch.batch = mixed_batch.batch[valid_batch_mask]\n",
    "            \n",
    "            # Mix labels\n",
    "            min_graphs = min(len(batch1.y), len(batch2.y))\n",
    "            y1_one_hot = F.one_hot(batch1.y[:min_graphs], num_classes=self.model1.num_class).float()\n",
    "            y2_one_hot = F.one_hot(batch2.y[:min_graphs], num_classes=self.model1.num_class).float()\n",
    "            mixed_targets = lam * y1_one_hot + (1 - lam) * y2_one_hot\n",
    "            \n",
    "            return mixed_batch, mixed_targets, lam\n",
    "        else:\n",
    "            # Fall back to label-only mixing\n",
    "            return self.mixup_data(batch1, batch2, alpha)\n",
    "    \n",
    "    def train_dividemix(self, dataloader, val_dataloader=None, epochs=100, clean_ratio_schedule=None):\n",
    "        \"\"\"Main DivideMix training loop with validation and BatchNorm safety\"\"\"\n",
    "        if clean_ratio_schedule is None:\n",
    "            # More conservative schedule for high noise (70%) and BatchNorm safety\n",
    "            clean_ratio_schedule = [0.25 + 0.25 * epoch / epochs for epoch in range(epochs)]\n",
    "        \n",
    "        # Warmup training\n",
    "        self.warmup_training(dataloader, val_dataloader, self.warmup_epochs)\n",
    "        \n",
    "        print(\"Starting DivideMix training...\")\n",
    "        for epoch in range(epochs):\n",
    "            current_clean_ratio = clean_ratio_schedule[epoch]\n",
    "            \n",
    "            # Divide samples using both models\n",
    "            clean_indices1, noisy_indices1 = self.divide_samples(\n",
    "                self.model2, dataloader, current_clean_ratio)\n",
    "            clean_indices2, noisy_indices2 = self.divide_samples(\n",
    "                self.model1, dataloader, current_clean_ratio)\n",
    "            \n",
    "            # Check if we have sufficient samples for BatchNorm\n",
    "            min_samples_needed = 8  # Minimum for stable BatchNorm\n",
    "            \n",
    "            if len(clean_indices1) < min_samples_needed:\n",
    "                print(f\"Warning: Too few clean samples for model1 ({len(clean_indices1)}), increasing clean ratio\")\n",
    "                current_clean_ratio = min(0.8, current_clean_ratio + 0.1)\n",
    "                clean_indices1, noisy_indices1 = self.divide_samples(\n",
    "                    self.model2, dataloader, current_clean_ratio)\n",
    "                    \n",
    "            if len(clean_indices2) < min_samples_needed:\n",
    "                print(f\"Warning: Too few clean samples for model2 ({len(clean_indices2)}), increasing clean ratio\")\n",
    "                current_clean_ratio = min(0.8, current_clean_ratio + 0.1)\n",
    "                clean_indices2, noisy_indices2 = self.divide_samples(\n",
    "                    self.model1, dataloader, current_clean_ratio)\n",
    "            \n",
    "            # Create dataloaders for clean and noisy samples with BatchNorm safety\n",
    "            clean_loader1 = self._create_subset_loader(clean_indices1, shuffle=True, min_batch_size=2)\n",
    "            noisy_loader1 = self._create_subset_loader(noisy_indices1, shuffle=True, min_batch_size=2)\n",
    "            clean_loader2 = self._create_subset_loader(clean_indices2, shuffle=True, min_batch_size=2)\n",
    "            noisy_loader2 = self._create_subset_loader(noisy_indices2, shuffle=True, min_batch_size=2)\n",
    "            \n",
    "            # Train model1\n",
    "            loss1 = self._train_epoch_dividemix(\n",
    "                self.model1, self.optimizer1, clean_loader1, noisy_loader1, clean_loader2)\n",
    "            \n",
    "            # Train model2\n",
    "            loss2 = self._train_epoch_dividemix(\n",
    "                self.model2, self.optimizer2, clean_loader2, noisy_loader2, clean_loader1)\n",
    "            \n",
    "            # Track training loss\n",
    "            avg_loss = (loss1 + loss2) / 2\n",
    "            self.train_history['loss'].append(avg_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if val_dataloader is not None:\n",
    "                val_acc = self.validate_models(val_dataloader, self.warmup_epochs + epoch)\n",
    "                \n",
    "                # Early stopping check\n",
    "                if self.should_early_stop():\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Clean ratio = {current_clean_ratio:.3f}, \"\n",
    "                      f\"Clean1: {len(clean_indices1)}, Clean2: {len(clean_indices2)}, \"\n",
    "                      f\"Train Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Load best models at the end\n",
    "        if val_dataloader is not None:\n",
    "            self.load_best_models()\n",
    "            print(f\"Loaded best models with validation accuracy: {self.best_val_acc:.4f}\")\n",
    "    \n",
    "    def _create_subset_loader(self, indices, batch_size=32, shuffle=False, min_batch_size=2):\n",
    "        \"\"\"Create dataloader for subset of indices with minimum batch size for BatchNorm\"\"\"\n",
    "        if len(indices) < min_batch_size:\n",
    "            return []  # Return empty if not enough samples for BatchNorm\n",
    "        \n",
    "        subset = Subset(self.dataset, indices)\n",
    "        \n",
    "        # Adjust batch size if subset is too small\n",
    "        effective_batch_size = min(batch_size, len(indices))\n",
    "        if effective_batch_size < min_batch_size:\n",
    "            effective_batch_size = min_batch_size\n",
    "        \n",
    "        return DataLoader(subset, batch_size=effective_batch_size, shuffle=shuffle, drop_last=True, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    def _train_epoch_dividemix(self, model, optimizer, clean_loader, noisy_loader, other_clean_loader):\n",
    "        \"\"\"Single epoch of DivideMix training with BatchNorm safety checks\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Check if we have valid loaders (not empty and sufficient samples)\n",
    "        has_clean = clean_loader and len(clean_loader) > 0\n",
    "        has_noisy = noisy_loader and len(noisy_loader) > 0\n",
    "        has_other_clean = other_clean_loader and len(other_clean_loader) > 0\n",
    "        \n",
    "        if not has_clean and not has_noisy:\n",
    "            print(\"Warning: No valid batches for training (insufficient samples for BatchNorm)\")\n",
    "            return 0.0\n",
    "        \n",
    "        # Convert to iterators for parallel iteration\n",
    "        clean_iter = iter(clean_loader) if has_clean else None\n",
    "        noisy_iter = iter(noisy_loader) if has_noisy else None\n",
    "        other_clean_iter = iter(other_clean_loader) if has_other_clean else None\n",
    "        \n",
    "        max_batches = max(len(clean_loader) if has_clean else 0,\n",
    "                         len(noisy_loader) if has_noisy else 0)\n",
    "        \n",
    "        if max_batches == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        for batch_idx in range(max_batches):\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "            \n",
    "            # Supervised loss on clean samples\n",
    "            if has_clean:\n",
    "                try:\n",
    "                    clean_batch = next(clean_iter)\n",
    "                    clean_batch = clean_batch.to(self.device)\n",
    "                    \n",
    "                    # Safety check for batch size\n",
    "                    if len(clean_batch.y) >= 2:\n",
    "                        clean_logits = model(clean_batch)\n",
    "                        clean_loss = F.cross_entropy(clean_logits, clean_batch.y)\n",
    "                        batch_loss += clean_loss\n",
    "                        \n",
    "                except StopIteration:\n",
    "                    clean_iter = iter(clean_loader)\n",
    "                    try:\n",
    "                        clean_batch = next(clean_iter)\n",
    "                        clean_batch = clean_batch.to(self.device)\n",
    "                        if len(clean_batch.y) >= 2:\n",
    "                            clean_logits = model(clean_batch)\n",
    "                            clean_loss = F.cross_entropy(clean_logits, clean_batch.y)\n",
    "                            batch_loss += clean_loss\n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "            \n",
    "            # Semi-supervised loss on noisy samples with MixUp\n",
    "            if has_noisy and has_other_clean:\n",
    "                try:\n",
    "                    noisy_batch = next(noisy_iter)\n",
    "                    other_clean_batch = next(other_clean_iter)\n",
    "                    \n",
    "                    noisy_batch = noisy_batch.to(self.device)\n",
    "                    other_clean_batch = other_clean_batch.to(self.device)\n",
    "                    \n",
    "                    # Safety check for batch sizes\n",
    "                    if len(noisy_batch.y) >= 1 and len(other_clean_batch.y) >= 1:\n",
    "                        # Apply MixUp between noisy and clean samples\n",
    "                        mixed_batch, mixed_targets, lam, selected_indices = self.mixup_data(noisy_batch, other_clean_batch)\n",
    "                        mixed_batch = mixed_batch.to(self.device)\n",
    "                        \n",
    "                        # Additional safety check for the mixed batch\n",
    "                        if len(selected_indices) >= 2:\n",
    "                            mixed_logits = model(mixed_batch)\n",
    "                            # Only use the selected subset for loss calculation\n",
    "                            mixed_logits_subset = mixed_logits[selected_indices]\n",
    "                            mixed_loss = -torch.sum(F.log_softmax(mixed_logits_subset, dim=1) * mixed_targets, dim=1).mean()\n",
    "                            \n",
    "                            batch_loss += 0.5 * mixed_loss  # Weight the semi-supervised loss\n",
    "                        \n",
    "                except StopIteration:\n",
    "                    # Reset iterators\n",
    "                    if has_noisy:\n",
    "                        noisy_iter = iter(noisy_loader)\n",
    "                    if has_other_clean:\n",
    "                        other_clean_iter = iter(other_clean_loader)\n",
    "            \n",
    "            if batch_loss > 0:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += batch_loss.item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        return total_loss / max(batch_count, 1)\n",
    "    \n",
    "    def get_final_clean_samples(self, dataloader, final_clean_ratio=0.5):\n",
    "        \"\"\"Get final set of clean samples using both models\"\"\"\n",
    "        # Adjusted for high noise rate\n",
    "        clean1, _ = self.divide_samples(self.model1, dataloader, final_clean_ratio)\n",
    "        clean2, _ = self.divide_samples(self.model2, dataloader, final_clean_ratio)\n",
    "        \n",
    "        # Take intersection for highest confidence\n",
    "        clean_intersection = list(set(clean1) & set(clean2))\n",
    "        \n",
    "        # If intersection is too small, take union (adjusted threshold for high noise)\n",
    "        if len(clean_intersection) < len(self.dataset) * 0.15:  # Lower threshold for 70% noise\n",
    "            clean_final = list(set(clean1) | set(clean2))\n",
    "        else:\n",
    "            clean_final = clean_intersection\n",
    "        \n",
    "        return clean_final\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation curves\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(self.train_history['loss'], label='Train Loss')\n",
    "        if self.val_history['loss']:\n",
    "            axes[0].plot(self.val_history['loss'], label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Accuracy plot\n",
    "        if self.val_history['accuracy']:\n",
    "            axes[1].plot(self.val_history['accuracy'], label='Val Accuracy')\n",
    "            axes[1].plot(self.val_history['f1'], label='Val F1')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Score')\n",
    "            axes[1].set_title('Validation Metrics')\n",
    "            axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup training phase...\n",
      "Epoch 0 - Val Loss: 7.5826, Val Acc: 0.3147, Val F1: 0.2236\n",
      "Warmup epoch 0/1 - Train Loss: 12.4585\n",
      "Starting DivideMix training...\n",
      "Epoch 1 - Val Loss: 1.8007, Val Acc: 0.3464, Val F1: 0.2882\n",
      "Epoch 0: Clean ratio = 0.250, Clean1: 1120, Clean2: 1120, Train Loss: 3.7356\n",
      "Epoch 2 - Val Loss: 1.7723, Val Acc: 0.3379, Val F1: 0.2922\n",
      "Epoch 3 - Val Loss: 1.7219, Val Acc: 0.3598, Val F1: 0.3038\n",
      "Epoch 4 - Val Loss: 1.7031, Val Acc: 0.3866, Val F1: 0.3225\n",
      "Epoch 5 - Val Loss: 1.6563, Val Acc: 0.3790, Val F1: 0.3118\n",
      "Epoch 6 - Val Loss: 1.6741, Val Acc: 0.3763, Val F1: 0.3198\n",
      "Epoch 7 - Val Loss: 1.6367, Val Acc: 0.3915, Val F1: 0.3462\n",
      "Epoch 8 - Val Loss: 1.7155, Val Acc: 0.3991, Val F1: 0.3388\n",
      "Epoch 9 - Val Loss: 1.5964, Val Acc: 0.4165, Val F1: 0.3701\n",
      "Epoch 10 - Val Loss: 1.6412, Val Acc: 0.4062, Val F1: 0.3516\n",
      "Epoch 11 - Val Loss: 1.6551, Val Acc: 0.3969, Val F1: 0.3524\n",
      "Epoch 10: Clean ratio = 0.275, Clean1: 1232, Clean2: 1232, Train Loss: 2.4914\n",
      "Epoch 12 - Val Loss: 1.6275, Val Acc: 0.4259, Val F1: 0.3668\n",
      "Epoch 13 - Val Loss: 1.5742, Val Acc: 0.4504, Val F1: 0.4116\n",
      "Epoch 14 - Val Loss: 1.6001, Val Acc: 0.4330, Val F1: 0.3923\n",
      "Epoch 15 - Val Loss: 1.5771, Val Acc: 0.4455, Val F1: 0.4041\n",
      "Epoch 16 - Val Loss: 1.5806, Val Acc: 0.4585, Val F1: 0.4180\n",
      "Epoch 17 - Val Loss: 1.5786, Val Acc: 0.4295, Val F1: 0.3934\n",
      "Epoch 18 - Val Loss: 1.5872, Val Acc: 0.4518, Val F1: 0.4043\n",
      "Epoch 19 - Val Loss: 1.6036, Val Acc: 0.4455, Val F1: 0.3964\n",
      "Epoch 20 - Val Loss: 1.5684, Val Acc: 0.4371, Val F1: 0.3973\n",
      "Epoch 21 - Val Loss: 1.5798, Val Acc: 0.4504, Val F1: 0.4019\n",
      "Epoch 20: Clean ratio = 0.300, Clean1: 1344, Clean2: 1344, Train Loss: 2.3810\n",
      "Epoch 22 - Val Loss: 1.6301, Val Acc: 0.4402, Val F1: 0.4117\n",
      "Epoch 23 - Val Loss: 1.6169, Val Acc: 0.4504, Val F1: 0.3984\n",
      "Epoch 24 - Val Loss: 1.5622, Val Acc: 0.4603, Val F1: 0.4175\n",
      "Epoch 25 - Val Loss: 1.5915, Val Acc: 0.4375, Val F1: 0.4020\n",
      "Epoch 26 - Val Loss: 1.5470, Val Acc: 0.4607, Val F1: 0.4199\n",
      "Epoch 27 - Val Loss: 1.5614, Val Acc: 0.4473, Val F1: 0.4188\n",
      "Epoch 28 - Val Loss: 1.5488, Val Acc: 0.4522, Val F1: 0.4123\n",
      "Epoch 29 - Val Loss: 1.5505, Val Acc: 0.4737, Val F1: 0.4280\n",
      "Epoch 30 - Val Loss: 1.5811, Val Acc: 0.4478, Val F1: 0.4186\n",
      "Epoch 31 - Val Loss: 1.5727, Val Acc: 0.4589, Val F1: 0.4245\n",
      "Epoch 30: Clean ratio = 0.325, Clean1: 1456, Clean2: 1456, Train Loss: 2.3322\n",
      "Epoch 32 - Val Loss: 1.5491, Val Acc: 0.4799, Val F1: 0.4341\n",
      "Epoch 33 - Val Loss: 1.5335, Val Acc: 0.4612, Val F1: 0.4341\n",
      "Epoch 34 - Val Loss: 1.5911, Val Acc: 0.4473, Val F1: 0.3972\n",
      "Epoch 35 - Val Loss: 1.5442, Val Acc: 0.4616, Val F1: 0.4290\n",
      "Epoch 36 - Val Loss: 1.5548, Val Acc: 0.4670, Val F1: 0.4311\n",
      "Epoch 37 - Val Loss: 1.5570, Val Acc: 0.4674, Val F1: 0.4342\n",
      "Epoch 38 - Val Loss: 1.5428, Val Acc: 0.4781, Val F1: 0.4472\n",
      "Epoch 39 - Val Loss: 1.5476, Val Acc: 0.4603, Val F1: 0.4266\n",
      "Epoch 40 - Val Loss: 1.5431, Val Acc: 0.4647, Val F1: 0.4373\n",
      "Epoch 41 - Val Loss: 1.5198, Val Acc: 0.4786, Val F1: 0.4487\n",
      "Epoch 40: Clean ratio = 0.350, Clean1: 1568, Clean2: 1568, Train Loss: 2.2901\n",
      "Epoch 42 - Val Loss: 1.5445, Val Acc: 0.4576, Val F1: 0.4279\n",
      "Epoch 43 - Val Loss: 1.5368, Val Acc: 0.4790, Val F1: 0.4499\n",
      "Epoch 44 - Val Loss: 1.5449, Val Acc: 0.4871, Val F1: 0.4546\n",
      "Epoch 45 - Val Loss: 1.5725, Val Acc: 0.4665, Val F1: 0.4313\n",
      "Epoch 46 - Val Loss: 1.5496, Val Acc: 0.4643, Val F1: 0.4454\n",
      "Epoch 47 - Val Loss: 1.5373, Val Acc: 0.4821, Val F1: 0.4514\n",
      "Epoch 48 - Val Loss: 1.5728, Val Acc: 0.4719, Val F1: 0.4478\n",
      "Epoch 49 - Val Loss: 1.5503, Val Acc: 0.4835, Val F1: 0.4475\n",
      "Epoch 50 - Val Loss: 1.5521, Val Acc: 0.4821, Val F1: 0.4652\n",
      "Epoch 51 - Val Loss: 1.5442, Val Acc: 0.4915, Val F1: 0.4644\n",
      "Epoch 50: Clean ratio = 0.375, Clean1: 1680, Clean2: 1680, Train Loss: 2.1926\n",
      "Epoch 52 - Val Loss: 1.5465, Val Acc: 0.4835, Val F1: 0.4546\n",
      "Epoch 53 - Val Loss: 1.5810, Val Acc: 0.4692, Val F1: 0.4340\n",
      "Epoch 54 - Val Loss: 1.5461, Val Acc: 0.4862, Val F1: 0.4605\n",
      "Epoch 55 - Val Loss: 1.5928, Val Acc: 0.4710, Val F1: 0.4362\n",
      "Epoch 56 - Val Loss: 1.5283, Val Acc: 0.4848, Val F1: 0.4583\n",
      "Epoch 57 - Val Loss: 1.5641, Val Acc: 0.4821, Val F1: 0.4528\n",
      "Epoch 58 - Val Loss: 1.5525, Val Acc: 0.4826, Val F1: 0.4558\n",
      "Epoch 59 - Val Loss: 1.5832, Val Acc: 0.4692, Val F1: 0.4375\n",
      "Epoch 60 - Val Loss: 1.5664, Val Acc: 0.4571, Val F1: 0.4403\n",
      "Epoch 61 - Val Loss: 1.5800, Val Acc: 0.4772, Val F1: 0.4465\n",
      "Epoch 60: Clean ratio = 0.400, Clean1: 1792, Clean2: 1792, Train Loss: 2.1624\n",
      "Epoch 62 - Val Loss: 1.5532, Val Acc: 0.4839, Val F1: 0.4535\n",
      "Epoch 63 - Val Loss: 1.5744, Val Acc: 0.4683, Val F1: 0.4431\n",
      "Epoch 64 - Val Loss: 1.5582, Val Acc: 0.4763, Val F1: 0.4558\n",
      "Epoch 65 - Val Loss: 1.5813, Val Acc: 0.4643, Val F1: 0.4396\n",
      "Epoch 66 - Val Loss: 1.5687, Val Acc: 0.4679, Val F1: 0.4490\n",
      "Epoch 67 - Val Loss: 1.5528, Val Acc: 0.4817, Val F1: 0.4615\n",
      "Epoch 68 - Val Loss: 1.5679, Val Acc: 0.4839, Val F1: 0.4700\n",
      "Epoch 69 - Val Loss: 1.5791, Val Acc: 0.4759, Val F1: 0.4495\n",
      "Epoch 70 - Val Loss: 1.5794, Val Acc: 0.4643, Val F1: 0.4432\n",
      "Epoch 71 - Val Loss: 1.5796, Val Acc: 0.4625, Val F1: 0.4501\n",
      "Early stopping at epoch 70\n",
      "Loaded best models with validation accuracy: 0.4915\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Get final clean samples (conservative for 70% noise)\u001b[39;00m\n\u001b[32m     28\u001b[39m clean_indices = trainer.get_final_clean_samples(train_loader, final_clean_ratio=\u001b[32m0.4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal clean dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clean_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Final evaluation on validation set\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create two identical models\n",
    "model1 = GNNContrastive(\n",
    "    gnn_type='gin',\n",
    "    num_class=6,\n",
    "    num_layer=args.num_layer,\n",
    "    emb_dim=args.emb_dim,\n",
    "    drop_ratio=args.drop_ratio,\n",
    "    virtual_node=True,\n",
    "    residual=True,\n",
    ").to(device)\n",
    "model2 = GNNContrastive(\n",
    "    gnn_type='gin',\n",
    "    num_class=6,\n",
    "    num_layer=args.num_layer,\n",
    "    emb_dim=args.emb_dim,\n",
    "    drop_ratio=args.drop_ratio,\n",
    "    virtual_node=True,\n",
    "    residual=True,\n",
    ").to(device)\n",
    "\n",
    "# Initialize trainer with validation\n",
    "trainer = DivideMixGraphTrainer(model1, model2, train_dataset, val_dataset)\n",
    "\n",
    "# Train with DivideMix and validation\n",
    "trainer.train_dividemix(train_loader, val_loader, epochs=100)\n",
    "\n",
    "# Get final clean samples (conservative for 70% noise)\n",
    "clean_indices = trainer.get_final_clean_samples(train_loader, final_clean_ratio=0.4)\n",
    "\n",
    "print(f\"Final clean dataset size: {len(clean_indices)} / {len(dataset)}\")\n",
    "\n",
    "# Final evaluation on validation set\n",
    "if val_loader:\n",
    "    val_loss1, val_acc1, val_f1_1 = trainer.evaluate_model(trainer.model1, val_loader)\n",
    "    val_loss2, val_acc2, val_f1_2 = trainer.evaluate_model(trainer.model2, val_loader)\n",
    "    \n",
    "    print(f\"Final Model 1 - Val Acc: {val_acc1:.4f}, Val F1: {val_f1_1:.4f}\")\n",
    "    print(f\"Final Model 2 - Val Acc: {val_acc2:.4f}, Val F1: {val_f1_2:.4f}\")\n",
    "    print(f\"Ensemble Average - Val Acc: {(val_acc1 + val_acc2)/2:.4f}\")\n",
    "\n",
    "# Create clean dataloader for further training if needed\n",
    "clean_loader__ = DataLoader(Subset(train_dataset, clean_indices), batch_size=32, shuffle=True)\n",
    "\n",
    "# Plot training history\n",
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final clean dataset size: 700 / 4480\n",
      "Final Model 1 - Val Acc: 0.4750, Val F1: 0.4542\n",
      "Final Model 2 - Val Acc: 0.4500, Val F1: 0.4459\n",
      "Ensemble Average - Val Acc: 0.4625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final clean dataset size: {len(clean_indices)} / {len(train_dataset)}\")\n",
    "\n",
    "# Final evaluation on validation set\n",
    "if val_loader:\n",
    "    val_loss1, val_acc1, val_f1_1 = trainer.evaluate_model(trainer.model1, val_loader)\n",
    "    val_loss2, val_acc2, val_f1_2 = trainer.evaluate_model(trainer.model2, val_loader)\n",
    "    \n",
    "    print(f\"Final Model 1 - Val Acc: {val_acc1:.4f}, Val F1: {val_f1_1:.4f}\")\n",
    "    print(f\"Final Model 2 - Val Acc: {val_acc2:.4f}, Val F1: {val_f1_2:.4f}\")\n",
    "    print(f\"Ensemble Average - Val Acc: {(val_acc1 + val_acc2)/2:.4f}\")\n",
    "\n",
    "# Create clean dataloader for further training if needed\n",
    "# clean_loader__ = DataLoader(Subset(train_dataset, clean_indices), batch_size=32, shuffle=True)\n",
    "\n",
    "# Plot training history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 35/35 [00:03<00:00,  9.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0000, Val Acc 0.4750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = evaluate(val_loader, trainer.model1, device, calculate_accuracy=True)\n",
    "print(f\"Val loss: {val_loss:.4f}, Val Acc {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 11/11 [00:03<00:00,  3.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0000, Val Acc 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_val_loss, clean_val_acc = evaluate(clean_val, trainer.model1, device, calculate_accuracy=True)\n",
    "print(f\"Val loss: {clean_val_loss:.4f}, Val Acc {clean_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/model_B_best.pth_pretrained.pth\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = f\"{checkpoint_path}_pretrained.pth\"\n",
    "torch.save(model.state_dict(), checkpoint_file)\n",
    "print(f\"Checkpoint saved at {checkpoint_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0, Loss: 10.4193\n",
      "Train Epoch 10, Loss: 0.6879\n",
      "Train Epoch 20, Loss: 0.6180\n"
     ]
    }
   ],
   "source": [
    "trainer.train_with_noise_robustness(clean_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GraphDatasetWithFeatures(args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 49/49 [00:04<00:00, 10.72batch/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate(test_loader, trainer.model1, device, calculate_accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /workspace/hackaton/submission/testset_B.csv\n"
     ]
    }
   ],
   "source": [
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best Epoch 47/50, Loss: 0.4592, Val loss: 0.5975, Train Acc: 0.7461, Val Acc 0.6941\n",
    "# needs retrain\n",
    "\n",
    "# num_layer=5,\n",
    "# emb_dim=300,\n",
    "# noize 0.3\n",
    "# without training on the full dataset\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/hackaton\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './submission' has been compressed into './submission.gz'\n"
     ]
    }
   ],
   "source": [
    "def gzip_folder(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Compresses an entire folder into a single .tar.gz file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder to compress.\n",
    "        output_file (str): Path to the output .tar.gz file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(folder_path, arcname=os.path.basename(folder_path))\n",
    "    print(f\"Folder '{folder_path}' has been compressed into '{output_file}'\")\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./submission\"            # Path to the folder you want to compress\n",
    "output_file = \"./submission.gz\"         # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 357374,
     "modelInstanceId": 340402,
     "sourceId": 417260,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
