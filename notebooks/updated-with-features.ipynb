{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "xSkgt1zf-raF",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "59f4a52f-5eb4-41e5-9fba-07432989fe78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
      "Collecting tqdm (from torch_geometric)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch_geometric)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m324.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Downloading multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: tqdm, propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.4 aiosignal-1.3.2 frozenlist-1.6.0 multidict-6.4.4 propcache-0.3.1 torch_geometric-2.6.1 tqdm-4.67.1 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.3)\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5oR2D2Us-xSQ",
    "outputId": "7086cadf-a7fe-4d75-f271-6339bee8164d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hackaton'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 81 (delta 4), reused 4 (delta 4), pack-reused 70 (from 1)\u001b[K\n",
      "Receiving objects: 100% (81/81), 105.83 MiB | 14.70 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n",
      "Updating files: 100% (38/38), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch baselineCe https://github.com/Graph-Classification-Noisy-Label/hackaton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tEhfPly6-7UK",
    "outputId": "3078ee06-6312-4fca-f5f9-888fa628c80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/hackaton\n"
     ]
    }
   ],
   "source": [
    "%cd hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "id": "PxBvwB0_6xI8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5933387c-2cfb-474f-d842-f36a3e2d2a73",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1oOxZgzm6GFEMqR9CE84syVkS3zDYm14G A\n",
      "Processing file 1KK7CrI83nsEBG4ft_BgBeB3FPfrtqfs6 test.json.gz\n",
      "Processing file 1HLDsT0NSttc9mI8obV4xsMdz0dXYsHg5 train.json.gz\n",
      "Retrieving folder 1fZMQBg3Zkd9k8D3ExvMHLPDZ-A4Uldz3 B\n",
      "Processing file 1nYi_Wj0c7FwMt1kNhenS5IOlfOAONUUO test.json.gz\n",
      "Processing file 14bnCHnrPdLQ9fmLbnoi3GKFrwfHy2JPN train.json.gz\n",
      "Retrieving folder 16I0ny5c6cuYncYTXNLro1i_bqUsJNeSc C\n",
      "Processing file 1HftG39UqLt5Zu7sYHcDl8GN6PqZV7i_M test.json.gz\n",
      "Processing file 1Q_QHBljWUS4ERBXbiZGQych-JKktRiMx train.json.gz\n",
      "Retrieving folder 1ToGahOt-r0Llmo6Bwq6WHnzq31Y6ixwm D\n",
      "Processing file 1yPgOhqJrmqP8eYfwCKgqdTz_blPZMzvj test.json.gz\n",
      "Processing file 1oZy8-zEj8heFGWAJ2loD6adqpdARVMlA train.json.gz\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1KK7CrI83nsEBG4ft_BgBeB3FPfrtqfs6\n",
      "From (redirected): https://drive.google.com/uc?id=1KK7CrI83nsEBG4ft_BgBeB3FPfrtqfs6&confirm=t&uuid=2a681391-795b-4acf-bb39-0ddfd5c425bc\n",
      "To: /workspace/hackaton/datasets/A/test.json.gz\n",
      "100%|██████████████████████████████████████| 92.4M/92.4M [00:03<00:00, 26.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1HLDsT0NSttc9mI8obV4xsMdz0dXYsHg5\n",
      "From (redirected): https://drive.google.com/uc?id=1HLDsT0NSttc9mI8obV4xsMdz0dXYsHg5&confirm=t&uuid=9a27ba6d-9922-4643-a567-5b71bdd94d64\n",
      "To: /workspace/hackaton/datasets/A/train.json.gz\n",
      "100%|████████████████████████████████████████| 465M/465M [00:14<00:00, 31.8MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1nYi_Wj0c7FwMt1kNhenS5IOlfOAONUUO\n",
      "From (redirected): https://drive.google.com/uc?id=1nYi_Wj0c7FwMt1kNhenS5IOlfOAONUUO&confirm=t&uuid=51b2f17b-c985-48e8-b322-5485558a14b4\n",
      "To: /workspace/hackaton/datasets/B/test.json.gz\n",
      "100%|██████████████████████████████████████| 63.0M/63.0M [00:01<00:00, 40.8MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14bnCHnrPdLQ9fmLbnoi3GKFrwfHy2JPN\n",
      "From (redirected): https://drive.google.com/uc?id=14bnCHnrPdLQ9fmLbnoi3GKFrwfHy2JPN&confirm=t&uuid=f592bd12-da0b-431c-8754-77c5e99cb4d0\n",
      "To: /workspace/hackaton/datasets/B/train.json.gz\n",
      "100%|████████████████████████████████████████| 223M/223M [00:09<00:00, 23.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1HftG39UqLt5Zu7sYHcDl8GN6PqZV7i_M\n",
      "From (redirected): https://drive.google.com/uc?id=1HftG39UqLt5Zu7sYHcDl8GN6PqZV7i_M&confirm=t&uuid=b27f203b-705c-41ba-891e-c6a847c4970d\n",
      "To: /workspace/hackaton/datasets/C/test.json.gz\n",
      "100%|██████████████████████████████████████| 60.5M/60.5M [00:02<00:00, 23.8MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Q_QHBljWUS4ERBXbiZGQych-JKktRiMx\n",
      "From (redirected): https://drive.google.com/uc?id=1Q_QHBljWUS4ERBXbiZGQych-JKktRiMx&confirm=t&uuid=0f2e4f85-81ac-451d-92d0-345595be40a6\n",
      "To: /workspace/hackaton/datasets/C/train.json.gz\n",
      "100%|████████████████████████████████████████| 308M/308M [00:11<00:00, 25.8MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1yPgOhqJrmqP8eYfwCKgqdTz_blPZMzvj\n",
      "From (redirected): https://drive.google.com/uc?id=1yPgOhqJrmqP8eYfwCKgqdTz_blPZMzvj&confirm=t&uuid=7ad81909-b75c-49dc-83a6-07a36e6a67b3\n",
      "To: /workspace/hackaton/datasets/D/test.json.gz\n",
      "100%|██████████████████████████████████████| 94.0M/94.0M [00:06<00:00, 14.4MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1oZy8-zEj8heFGWAJ2loD6adqpdARVMlA\n",
      "From (redirected): https://drive.google.com/uc?id=1oZy8-zEj8heFGWAJ2loD6adqpdARVMlA&confirm=t&uuid=37c559e4-cf70-49b8-b239-92e413cee8bf\n",
      "To: /workspace/hackaton/datasets/D/train.json.gz\n",
      "100%|████████████████████████████████████████| 439M/439M [00:15<00:00, 27.6MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "# !gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets\n",
    "!gdown --folder https://drive.google.com/drive/folders/1vZVCPbux6brOnkfqil9CIj8a4iHlUwHV -O datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lAQuCuIoBbq5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Load utility functions from cloned repository\n",
    "from src.loadData import GraphDataset\n",
    "from src.utils import set_seed\n",
    "from src.models import GNN\n",
    "import argparse\n",
    "\n",
    "# Set the random seed\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "class GraphDatasetWithFeatures(Dataset):\n",
    "    def __init__(self, filename, transform=None, pre_transform=None):\n",
    "        self.raw = filename\n",
    "        self.num_graphs, self.graphs_dicts = self._count_graphs()\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "\n",
    "    def len(self):\n",
    "        return self.num_graphs\n",
    "\n",
    "    def get(self, idx):\n",
    "        return dictToGraphObjectWithFeatures(self.graphs_dicts[idx])\n",
    "\n",
    "    def _count_graphs(self):\n",
    "        with gzip.open(self.raw, \"rt\", encoding=\"utf-8\") as f:\n",
    "            graphs_dicts = json.load(f)\n",
    "            return len(graphs_dicts), graphs_dicts\n",
    "\n",
    "\n",
    "def dictToGraphObjectWithFeatures(graph_dict):\n",
    "    edge_index = torch.tensor(graph_dict[\"edge_index\"], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(graph_dict[\"edge_attr\"], dtype=torch.float) if graph_dict.get(\"edge_attr\") else None\n",
    "    num_nodes = graph_dict[\"num_nodes\"]\n",
    "    y = torch.tensor(graph_dict[\"y\"][0], dtype=torch.long) if graph_dict.get(\"y\") is not None else None\n",
    "\n",
    "    node_features = None\n",
    "    if edge_attr is not None:\n",
    "        node_feature_dim = edge_attr.shape[1]\n",
    "        node_features = torch.zeros((num_nodes, node_feature_dim), dtype=torch.float)\n",
    "\n",
    "        row, col = edge_index\n",
    "        node_features.index_add_(0, row, edge_attr)\n",
    "        node_features.index_add_(0, col, edge_attr)\n",
    "\n",
    "        deg = degree(edge_index[0], num_nodes=num_nodes, dtype=torch.float)\n",
    "        node_features = node_features / deg.unsqueeze(1)\n",
    "\n",
    "    graph_features = torch.tensor(graph_dict[\"graph_features\"], dtype=torch.float) if graph_dict.get(\"graph_features\") else None\n",
    "\n",
    "    return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, num_nodes=num_nodes, y=y, graph_x=graph_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, GCNConv, global_add_pool\n",
    "\n",
    "class GNN_node_Virtualnode_With_Features(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, drop_ratio=0.5, JK=\"last\", residual=False, gnn_type='gin', input_dim=None):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality (output dimension of the first layer, and hidden dim for subsequent layers)\n",
    "            input_dim (int): the actual dimension of the input node features (e.g., 7 for OGB-PPA derived features)\n",
    "        '''\n",
    "        super(GNN_node_Virtualnode_With_Features, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.residual = residual\n",
    "        self.emb_dim = emb_dim # This is the hidden dimension of the GNN layers\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        if input_dim is None:\n",
    "            raise ValueError(\"input_dim must be provided for GNN_node_Virtualnode (e.g., 7 for OGB-PPA).\")\n",
    "        self.node_encoder = torch.nn.Linear(input_dim, emb_dim)\n",
    "\n",
    "        ### set the initial virtual node embedding to 0.\n",
    "        self.virtualnode_embedding = torch.nn.Embedding(1, emb_dim)\n",
    "        torch.nn.init.constant_(self.virtualnode_embedding.weight.data, 0)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.mlp_virtualnode_list = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layer):\n",
    "            # PyG's GINConv and GCNConv expect `in_channels` and `out_channels`\n",
    "            # For GINConv, it takes an MLP, so we pass `emb_dim` as the input/output dim of the MLP.\n",
    "            # For GCNConv, it takes `in_channels` and `out_channels`.\n",
    "            if gnn_type == 'gin':\n",
    "                # The MLP passed to GINConv should map from emb_dim to emb_dim\n",
    "                self.convs.append(GINConv(torch.nn.Sequential(\n",
    "                    torch.nn.Linear(emb_dim, 2 * emb_dim),\n",
    "                    torch.nn.BatchNorm1d(2 * emb_dim),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(2 * emb_dim, emb_dim)\n",
    "                )))\n",
    "            elif gnn_type == 'gcn':\n",
    "                self.convs.append(GCNConv(emb_dim, emb_dim))\n",
    "            else:\n",
    "                raise ValueError('Undefined GNN type called {}'.format(gnn_type))\n",
    "\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "        for layer in range(num_layer - 1):\n",
    "            self.mlp_virtualnode_list.append(torch.nn.Sequential(\n",
    "                torch.nn.Linear(emb_dim, 2*emb_dim),\n",
    "                torch.nn.BatchNorm1d(2*emb_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(2*emb_dim, emb_dim),\n",
    "                torch.nn.BatchNorm1d(emb_dim),\n",
    "                torch.nn.ReLU() # ReLU here is consistent with the original\n",
    "            ))\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        x, edge_index, edge_attr, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.batch\n",
    "\n",
    "        if x is None:\n",
    "            raise ValueError(\"batched_data.x (node features) is None. Ensure your dataset correctly derives them.\")\n",
    "        h = self.node_encoder(x)\n",
    "        h_list = [h]\n",
    "\n",
    "        virtualnode_embedding = self.virtualnode_embedding(torch.zeros(batch[-1].item() + 1, dtype=torch.long, device=edge_index.device))\n",
    "\n",
    "        for layer in range(self.num_layer):\n",
    "            h_list[layer] = h_list[layer] + virtualnode_embedding[batch]\n",
    "\n",
    "            if self.convs[layer].__class__.__name__ == 'GINConv':\n",
    "                h = self.convs[layer](h_list[layer], edge_index) \n",
    "            elif self.convs[layer].__class__.__name__ == 'GCNConv':\n",
    "                h = self.convs[layer](h_list[layer], edge_index, edge_weight=edge_attr if edge_attr is not None else None) # GCNConv can take edge_weight\n",
    "            else:\n",
    "                h = self.convs[layer](h_list[layer], edge_index, edge_attr) \n",
    "\n",
    "            h = self.batch_norms[layer](h)\n",
    "\n",
    "            if layer == self.num_layer - 1:\n",
    "                h = F.dropout(h, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)\n",
    "\n",
    "            if self.residual:\n",
    "                h = h + h_list[layer] # Residual connection\n",
    "\n",
    "            h_list.append(h)\n",
    "\n",
    "            ### update the virtual nodes\n",
    "            if layer < self.num_layer - 1:\n",
    "                virtualnode_embedding_temp = global_add_pool(h_list[layer], batch) + virtualnode_embedding\n",
    "                ### transform virtual nodes using MLP\n",
    "\n",
    "                if self.residual:\n",
    "                    virtualnode_embedding = virtualnode_embedding + F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio, training=self.training)\n",
    "                else:\n",
    "                    virtualnode_embedding = F.dropout(self.mlp_virtualnode_list[layer](virtualnode_embedding_temp), self.drop_ratio, training=self.training)\n",
    "\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"sum\":\n",
    "            node_representation = 0\n",
    "            # Iterate over all h_list (including initial embedding)\n",
    "            for h_rep in h_list:\n",
    "                node_representation += h_rep\n",
    "        else:\n",
    "            raise ValueError(\"Undefined JK type: {}\".format(self.JK))\n",
    "\n",
    "        return node_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "from src.conv import GNN_node, GNN_node_Virtualnode\n",
    "\n",
    "class GNN_With_Features(torch.nn.Module):\n",
    "    def __init__(self, num_class, num_layer=5, emb_dim=None, # <--- emb_dim can be dynamically set\n",
    "                 gnn_type='gin', virtual_node=True, residual=False,\n",
    "                 drop_ratio=0.5, JK=\"last\", graph_pooling=\"mean\",\n",
    "                 num_graph_features=0):\n",
    "        super(GNN_With_Features, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        self.num_class = num_class\n",
    "        self.graph_pooling = graph_pooling\n",
    "        self.num_graph_features = num_graph_features\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        # If emb_dim is not provided, we will determine it from the first batch\n",
    "        # This requires `self.gnn_node` to handle None initially or be initialized later\n",
    "        # A safer approach is to pass the expected input dim to GNN_node\n",
    "        # For OGB-PPA, the input node feature dimension is 7 (from edge_attr)\n",
    "        self.input_node_feat_dim = 7 # <--- Explicitly set for OGB-PPA derived features\n",
    "        self.emb_dim = emb_dim if emb_dim is not None else self.input_node_feat_dim # Use derived dim if not specified\n",
    "\n",
    "        # GNN_node now needs to know the input dimension (self.input_node_feat_dim)\n",
    "        if virtual_node:\n",
    "            self.gnn_node = GNN_node_Virtualnode_With_Features(\n",
    "                num_layer,\n",
    "                self.emb_dim,\n",
    "                JK=JK,\n",
    "                drop_ratio=drop_ratio,\n",
    "                residual=residual,\n",
    "                gnn_type=gnn_type,\n",
    "                input_dim=self.input_node_feat_dim) # <--- Pass input_dim\n",
    "        else:\n",
    "            self.gnn_node = GNN_node(num_layer, self.emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual, gnn_type=gnn_type,\n",
    "                                    input_dim=self.input_node_feat_dim) # <--- Pass input_dim\n",
    "\n",
    "        ### Pooling function\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            # Gate network input dimension needs to match emb_dim\n",
    "            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(self.emb_dim, 2*self.emb_dim), torch.nn.BatchNorm1d(2*self.emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*self.emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(self.emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        # Final linear layer\n",
    "        if graph_pooling == \"set2set\":\n",
    "            pooled_dim = 2 * self.emb_dim\n",
    "        else:\n",
    "            pooled_dim = self.emb_dim\n",
    "\n",
    "        self.graph_pred_linear = torch.nn.Linear(pooled_dim + self.num_graph_features, self.num_class)\n",
    "\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # The GNN_node module expects batched_data.x for node features\n",
    "        # Your dictToGraphObject now correctly sets batched_data.x\n",
    "        h_node = self.gnn_node(batched_data)\n",
    "        h_graph = self.pool(h_node, batched_data.batch)\n",
    "\n",
    "        if self.num_graph_features > 0:\n",
    "            if not hasattr(batched_data, 'graph_x') or batched_data.graph_x is None:\n",
    "                raise ValueError(\"num_graph_features is set > 0, but 'graph_x' attribute not found in batched_data.\")\n",
    "            if batched_data.graph_x.shape[0] != h_graph.shape[0]:\n",
    "                raise ValueError(f\"Batch size of graph_x ({batched_data.graph_x.shape[0]}) does not match pooled graph embeddings ({h_graph.shape[0]}).\")\n",
    "\n",
    "            h_graph = torch.cat([h_graph, batched_data.graph_x], dim=-1)\n",
    "\n",
    "        return self.graph_pred_linear(h_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GeneralizedCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\n",
    "    2018 - Zhang et al. - https://arxiv.org/pdf/1805.07836.pdf\n",
    "    \n",
    "    Args:\n",
    "        q (float): Robustness parameter. q=0 gives standard cross-entropy, q=1 gives MAE.\n",
    "                  Recommended range: [0.1, 0.7] for noise robustness.\n",
    "        reduction (str): Specifies the reduction to apply to the output.\n",
    "        eps (float): Small value to prevent numerical instability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, q=0.3, reduction='mean', eps=1e-7):\n",
    "        super(GeneralizedCrossEntropyLoss, self).__init__()\n",
    "        self.q = q\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Tensor of shape (N, C) containing raw logits\n",
    "            targets: Tensor of shape (N,) containing class indices\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        y_pred = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert targets to one-hot encoding\n",
    "        y_true = F.one_hot(targets, num_classes=logits.size(-1)).float()\n",
    "        \n",
    "        # Compute GCE loss with clamping for numerical stability\n",
    "        # Clamp predictions to avoid numerical issues\n",
    "        y_pred = torch.clamp(y_pred, min=self.eps, max=1.0 - self.eps)\n",
    "        \n",
    "        # Compute the sum of y_true * y_pred along class dimension\n",
    "        pred_sum = torch.sum(y_true * y_pred, dim=-1)\n",
    "        \n",
    "        # Clamp again before taking the power to ensure stability\n",
    "        pred_sum = torch.clamp(pred_sum, min=self.eps)\n",
    "        \n",
    "        # GCE loss: (1 - pred_sum^q) / q\n",
    "        if self.q == 0:\n",
    "            # Special case: standard cross-entropy (limit as q->0)\n",
    "            loss = -torch.log(pred_sum)\n",
    "        else:\n",
    "            intermed = torch.pow(pred_sum, self.q)\n",
    "            loss = (1 - intermed) / self.q\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        else:  # 'none'\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "x1OnGq_nCmTr"
   },
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    # Save checkpoints if required\n",
    "    if save_checkpoints:\n",
    "        checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_file)\n",
    "        print(f\"Checkpoint saved at {checkpoint_file}\")\n",
    "\n",
    "    return total_loss / len(data_loader),  correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, device, criterion=None, calculate_accuracy=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            \n",
    "            if calculate_accuracy:\n",
    "                correct += (pred == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "                if criterion is not None:\n",
    "                    total_loss += criterion(output, data.y).item()\n",
    "\n",
    "            else:\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "    \n",
    "    if calculate_accuracy:\n",
    "        accuracy = correct / total\n",
    "        return  total_loss / len(data_loader), accuracy\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_path):\n",
    "    # script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    script_dir = os.getcwd()\n",
    "    submission_folder = os.path.join(script_dir, \"submission\")\n",
    "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
    "    \n",
    "    os.makedirs(submission_folder, exist_ok=True)\n",
    "    \n",
    "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
    "    \n",
    "    test_graph_ids = list(range(len(predictions)))\n",
    "    output_df = pd.DataFrame({\n",
    "        \"id\": test_graph_ids,\n",
    "        \"pred\": predictions\n",
    "    })\n",
    "    \n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(train_losses, train_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy per Epoch')\n",
    "\n",
    "    # Save plots in the current directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "script_dir = os.getcwd()\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_checkpoints = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_name,\n",
    "            noise_ratio=0.2,\n",
    "            num_layer=5,\n",
    "            emb_dim=256,\n",
    "            drop_ratio=0.2,\n",
    "            batch_size=32,\n",
    "    ):\n",
    "        self.train_path = f\"datasets/{dataset_name}/train.json.gz\"\n",
    "        self.test_path = f\"datasets/{dataset_name}/test.json.gz\"\n",
    "\n",
    "        self.noise_ratio = noise_ratio\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "        self.emb_dim = emb_dim\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.batch_size = batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_name = os.path.basename(os.path.dirname(args.test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
    "log_file = os.path.join(logs_folder, \"training.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())  # Console output as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path relative to the script's directory\n",
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
    "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
    "os.makedirs(checkpoints_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model for inference\n",
    "if os.path.exists(checkpoint_path) and not args.train_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"Loaded best model from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_With_Features(\n",
    "    gnn_type='gin',\n",
    "    num_class=6,\n",
    "    num_layer=args.num_layer,\n",
    "    emb_dim=args.emb_dim,\n",
    "    drop_ratio=args.drop_ratio,\n",
    "    virtual_node=True,\n",
    "    residual=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = GeneralizedCrossEntropyLoss(args.noise_ratio)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = GraphDatasetWithFeatures(args.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(0.2 * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(12)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.57batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.83batch/s]\n",
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n",
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n",
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n",
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n",
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n",
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.3773, Val loss: 1.5184, Train Acc: 0.3479, Val Acc 0.3813\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.33batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.46batch/s]\n",
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n",
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n",
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n",
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n",
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n",
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 1.3245, Val loss: 1.1622, Train Acc: 0.4190, Val Acc 0.4329\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.93batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.72batch/s]\n",
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n",
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n",
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n",
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n",
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n",
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 1.1493, Val loss: 1.2755, Train Acc: 0.4688, Val Acc 0.4523\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.20batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.94batch/s]\n",
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n",
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n",
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n",
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n",
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n",
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 1.1296, Val loss: 1.1839, Train Acc: 0.4770, Val Acc 0.4854\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.18batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.55batch/s]\n",
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n",
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n",
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n",
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n",
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n",
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 1.0897, Val loss: 1.0562, Train Acc: 0.4895, Val Acc 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.00batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.73batch/s]\n",
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n",
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n",
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n",
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n",
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n",
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 1.1653, Val loss: 1.0873, Train Acc: 0.4658, Val Acc 0.5054\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 18.00batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.75batch/s]\n",
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n",
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n",
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n",
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n",
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n",
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 1.0698, Val loss: 1.0698, Train Acc: 0.4994, Val Acc 0.4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.51batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.60batch/s]\n",
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n",
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n",
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n",
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n",
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n",
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 1.0379, Val loss: 0.9631, Train Acc: 0.5119, Val Acc 0.5491\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.14batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.81batch/s]\n",
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n",
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n",
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n",
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n",
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n",
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.9896, Val loss: 0.9192, Train Acc: 0.5260, Val Acc 0.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.78batch/s]\n",
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n",
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n",
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n",
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n",
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n",
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.9342, Val loss: 0.8919, Train Acc: 0.5387, Val Acc 0.5730\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.30batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.81batch/s]\n",
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n",
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n",
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n",
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n",
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n",
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.8235, Val loss: 0.8215, Train Acc: 0.5750, Val Acc 0.5934\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.29batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.65batch/s]\n",
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n",
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n",
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n",
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n",
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n",
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.8055, Val loss: 0.8609, Train Acc: 0.5776, Val Acc 0.5467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.08batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.91batch/s]\n",
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n",
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n",
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n",
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n",
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n",
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.7625, Val loss: 0.8202, Train Acc: 0.6017, Val Acc 0.5749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.31batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.90batch/s]\n",
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n",
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n",
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n",
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n",
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n",
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.7551, Val loss: 0.7831, Train Acc: 0.6059, Val Acc 0.6075\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.15batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.66batch/s]\n",
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n",
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n",
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n",
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n",
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n",
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 0.7553, Val loss: 0.7448, Train Acc: 0.6075, Val Acc 0.6187\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.17batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.63batch/s]\n",
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n",
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n",
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n",
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n",
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n",
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 0.7292, Val loss: 0.7724, Train Acc: 0.6134, Val Acc 0.6026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.14batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.71batch/s]\n",
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n",
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n",
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n",
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n",
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n",
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 0.7214, Val loss: 0.7411, Train Acc: 0.6184, Val Acc 0.6109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.30batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.58batch/s]\n",
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n",
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n",
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n",
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n",
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n",
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 0.7057, Val loss: 0.7343, Train Acc: 0.6257, Val Acc 0.6172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:13<00:00, 19.00batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.55batch/s]\n",
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n",
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n",
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n",
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n",
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n",
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 0.7026, Val loss: 0.7248, Train Acc: 0.6265, Val Acc 0.6381\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 18.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.75batch/s]\n",
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n",
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n",
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n",
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n",
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n",
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.6968, Val loss: 0.7380, Train Acc: 0.6227, Val Acc 0.6396\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.63batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.52batch/s]\n",
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n",
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n",
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n",
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n",
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n",
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 0.6334, Val loss: 0.6678, Train Acc: 0.6564, Val Acc 0.6586\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.85batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.65batch/s]\n",
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n",
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n",
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n",
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n",
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n",
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 0.6179, Val loss: 0.6798, Train Acc: 0.6622, Val Acc 0.6483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.17batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.71batch/s]\n",
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n",
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n",
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n",
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n",
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n",
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 0.6050, Val loss: 0.6632, Train Acc: 0.6711, Val Acc 0.6620\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.30batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.40batch/s]\n",
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n",
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n",
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n",
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n",
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n",
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 0.5986, Val loss: 0.6771, Train Acc: 0.6734, Val Acc 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.46batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.71batch/s]\n",
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n",
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n",
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n",
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n",
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n",
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 0.5843, Val loss: 0.6622, Train Acc: 0.6813, Val Acc 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.12batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.81batch/s]\n",
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n",
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n",
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n",
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n",
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n",
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 0.5783, Val loss: 0.6298, Train Acc: 0.6815, Val Acc 0.6756\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.23batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.13batch/s]\n",
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n",
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n",
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n",
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n",
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n",
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 0.5788, Val loss: 0.6825, Train Acc: 0.6857, Val Acc 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.46batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.68batch/s]\n",
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n",
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n",
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n",
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n",
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n",
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 0.5662, Val loss: 0.6376, Train Acc: 0.6859, Val Acc 0.6663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.78batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.46batch/s]\n",
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n",
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n",
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n",
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n",
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n",
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 0.5637, Val loss: 0.6133, Train Acc: 0.6937, Val Acc 0.6877\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.48batch/s]\n",
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n",
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n",
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n",
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n",
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n",
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 0.5452, Val loss: 0.6109, Train Acc: 0.7011, Val Acc 0.6552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:13<00:00, 18.49batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.71batch/s]\n",
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n",
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n",
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n",
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n",
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n",
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 0.5127, Val loss: 0.5837, Train Acc: 0.7170, Val Acc 0.6907\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.99batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.33batch/s]\n",
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n",
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n",
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n",
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n",
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n",
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 0.5031, Val loss: 0.6250, Train Acc: 0.7218, Val Acc 0.6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.10batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 10.96batch/s]\n",
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n",
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n",
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n",
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n",
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n",
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 0.4913, Val loss: 0.6049, Train Acc: 0.7305, Val Acc 0.6576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.08batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.00batch/s]\n",
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n",
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n",
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n",
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n",
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n",
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 0.4925, Val loss: 0.6220, Train Acc: 0.7310, Val Acc 0.6892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.91batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.42batch/s]\n",
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n",
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n",
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n",
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n",
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n",
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 0.4882, Val loss: 0.5890, Train Acc: 0.7377, Val Acc 0.6911\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.09batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.37batch/s]\n",
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n",
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n",
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n",
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n",
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n",
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 0.4795, Val loss: 0.6375, Train Acc: 0.7392, Val Acc 0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.48batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.90batch/s]\n",
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n",
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n",
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n",
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n",
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n",
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 0.4728, Val loss: 0.5957, Train Acc: 0.7422, Val Acc 0.6829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 18.17batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.01batch/s]\n",
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n",
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n",
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n",
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n",
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n",
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 0.4579, Val loss: 0.5900, Train Acc: 0.7511, Val Acc 0.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.81batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.72batch/s]\n",
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n",
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n",
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n",
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n",
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n",
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 0.4539, Val loss: 0.6044, Train Acc: 0.7483, Val Acc 0.6887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.74batch/s]\n",
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n",
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n",
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n",
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n",
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n",
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.4549, Val loss: 0.5937, Train Acc: 0.7490, Val Acc 0.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.73batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.79batch/s]\n",
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n",
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n",
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n",
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n",
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n",
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 0.4254, Val loss: 0.5816, Train Acc: 0.7695, Val Acc 0.7004\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.61batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.03batch/s]\n",
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n",
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n",
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n",
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n",
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n",
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 0.4088, Val loss: 0.5771, Train Acc: 0.7800, Val Acc 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.93batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.84batch/s]\n",
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n",
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n",
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n",
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n",
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n",
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 0.4150, Val loss: 0.5898, Train Acc: 0.7768, Val Acc 0.6994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.21batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.37batch/s]\n",
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n",
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n",
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n",
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n",
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n",
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 0.4098, Val loss: 0.5948, Train Acc: 0.7845, Val Acc 0.6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:13<00:00, 18.79batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.43batch/s]\n",
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n",
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n",
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n",
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n",
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n",
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Loss: 0.3974, Val loss: 0.6043, Train Acc: 0.7870, Val Acc 0.6775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.24batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.44batch/s]\n",
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n",
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n",
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n",
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n",
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n",
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Loss: 0.4006, Val loss: 0.6137, Train Acc: 0.7856, Val Acc 0.6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.72batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.17batch/s]\n",
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n",
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n",
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n",
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n",
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n",
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Loss: 0.3930, Val loss: 0.6290, Train Acc: 0.7918, Val Acc 0.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.82batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.10batch/s]\n",
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n",
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n",
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n",
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n",
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n",
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Loss: 0.3984, Val loss: 0.6232, Train Acc: 0.7844, Val Acc 0.6809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.67batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.94batch/s]\n",
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n",
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n",
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n",
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n",
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n",
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Loss: 0.3880, Val loss: 0.5995, Train Acc: 0.7902, Val Acc 0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.41batch/s]\n",
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n",
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n",
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n",
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n",
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n",
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 0.3834, Val loss: 0.6112, Train Acc: 0.7968, Val Acc 0.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 18.06batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.69batch/s]\n",
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n",
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n",
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n",
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n",
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n",
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Loss: 0.3699, Val loss: 0.6194, Train Acc: 0.8031, Val Acc 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:13<00:00, 19.41batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.68batch/s]\n",
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n",
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n",
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n",
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n",
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n",
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Loss: 0.3675, Val loss: 0.6293, Train Acc: 0.8008, Val Acc 0.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.90batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.49batch/s]\n",
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n",
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n",
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n",
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n",
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n",
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Loss: 0.3680, Val loss: 0.6087, Train Acc: 0.8091, Val Acc 0.6819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.27batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.98batch/s]\n",
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n",
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n",
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n",
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n",
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n",
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Loss: 0.3581, Val loss: 0.6085, Train Acc: 0.8079, Val Acc 0.6975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.62batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.91batch/s]\n",
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n",
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n",
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n",
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n",
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n",
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Loss: 0.3529, Val loss: 0.6290, Train Acc: 0.8149, Val Acc 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.70batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.75batch/s]\n",
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n",
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n",
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n",
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n",
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n",
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Loss: 0.3573, Val loss: 0.6433, Train Acc: 0.8108, Val Acc 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.12batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.01batch/s]\n",
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n",
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n",
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n",
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n",
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n",
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Loss: 0.3517, Val loss: 0.6216, Train Acc: 0.8152, Val Acc 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.06batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.48batch/s]\n",
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n",
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n",
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n",
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n",
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n",
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Loss: 0.3526, Val loss: 0.6296, Train Acc: 0.8126, Val Acc 0.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.38batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.52batch/s]\n",
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n",
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n",
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n",
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n",
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n",
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Loss: 0.3466, Val loss: 0.6231, Train Acc: 0.8204, Val Acc 0.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_60.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.88batch/s]\n",
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n",
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n",
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n",
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n",
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n",
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 0.3395, Val loss: 0.6314, Train Acc: 0.8253, Val Acc 0.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.14batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.87batch/s]\n",
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n",
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n",
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n",
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n",
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n",
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Loss: 0.3345, Val loss: 0.6241, Train Acc: 0.8292, Val Acc 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 18.02batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.63batch/s]\n",
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n",
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n",
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n",
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n",
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n",
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Loss: 0.3296, Val loss: 0.6357, Train Acc: 0.8298, Val Acc 0.6795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.45batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.52batch/s]\n",
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n",
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n",
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n",
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n",
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n",
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Loss: 0.3295, Val loss: 0.6546, Train Acc: 0.8281, Val Acc 0.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:13<00:00, 18.55batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.66batch/s]\n",
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n",
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n",
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n",
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n",
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n",
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Loss: 0.3275, Val loss: 0.6501, Train Acc: 0.8305, Val Acc 0.6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.98batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.49batch/s]\n",
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n",
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n",
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n",
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n",
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n",
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Loss: 0.3240, Val loss: 0.6446, Train Acc: 0.8275, Val Acc 0.6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 18.06batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.71batch/s]\n",
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n",
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n",
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n",
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n",
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n",
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Loss: 0.3253, Val loss: 0.6415, Train Acc: 0.8306, Val Acc 0.6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.17batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.54batch/s]\n",
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n",
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n",
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n",
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n",
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n",
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Loss: 0.3284, Val loss: 0.6583, Train Acc: 0.8283, Val Acc 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.03batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.56batch/s]\n",
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n",
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n",
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n",
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n",
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n",
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Loss: 0.3277, Val loss: 0.6341, Train Acc: 0.8286, Val Acc 0.6873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.68batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.41batch/s]\n",
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n",
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n",
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n",
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n",
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n",
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Loss: 0.3273, Val loss: 0.6565, Train Acc: 0.8275, Val Acc 0.6751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_70.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.76batch/s]\n",
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n",
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n",
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n",
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n",
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n",
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 0.3205, Val loss: 0.6441, Train Acc: 0.8332, Val Acc 0.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.83batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 10.99batch/s]\n",
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n",
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n",
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n",
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n",
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n",
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Loss: 0.3205, Val loss: 0.6400, Train Acc: 0.8345, Val Acc 0.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.26batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.58batch/s]\n",
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n",
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n",
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n",
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n",
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n",
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Loss: 0.3120, Val loss: 0.6496, Train Acc: 0.8423, Val Acc 0.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.12batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.22batch/s]\n",
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n",
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n",
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n",
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n",
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n",
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Loss: 0.3166, Val loss: 0.6424, Train Acc: 0.8363, Val Acc 0.6809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.20batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.03batch/s]\n",
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n",
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n",
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n",
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n",
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n",
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Loss: 0.3181, Val loss: 0.6546, Train Acc: 0.8310, Val Acc 0.6795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.39batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.28batch/s]\n",
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n",
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n",
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n",
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n",
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n",
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Loss: 0.3141, Val loss: 0.6552, Train Acc: 0.8396, Val Acc 0.6809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 17.00batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.78batch/s]\n",
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n",
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n",
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n",
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n",
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n",
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Loss: 0.3122, Val loss: 0.6536, Train Acc: 0.8357, Val Acc 0.6804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.75batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.29batch/s]\n",
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n",
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n",
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n",
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n",
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n",
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Loss: 0.3105, Val loss: 0.6433, Train Acc: 0.8412, Val Acc 0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:15<00:00, 16.71batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.47batch/s]\n",
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n",
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n",
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n",
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n",
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n",
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Loss: 0.3097, Val loss: 0.6502, Train Acc: 0.8433, Val Acc 0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.71batch/s]\n",
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 11.68batch/s]\n",
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n",
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n",
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n",
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n",
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n",
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Loss: 0.3124, Val loss: 0.6842, Train Acc: 0.8397, Val Acc 0.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 257/257 [00:14<00:00, 17.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_80.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 65/65 [00:05<00:00, 12.11batch/s]\n",
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n",
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n",
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n",
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n",
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n",
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 0.3165, Val loss: 0.6603, Train Acc: 0.8354, Val Acc 0.6751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs:  23%|██▎       | 60/257 [00:07<00:23,  8.44batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[172]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     checkpoint_intervals = [num_epochs]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     train_loss, train_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_intervals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoints_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_dir_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=\u001b[38;5;28;01mTrue\u001b[39;00m, criterion=criterion)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch, scaler)\u001b[39m\n\u001b[32m      7\u001b[39m data = data.to(device)\n\u001b[32m      8\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m loss = criterion(output, data.y)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mGNN_With_Features.forward\u001b[39m\u001b[34m(self, batched_data)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_data):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# The GNN_node module expects batched_data.x for node features\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Your dictToGraphObject now correctly sets batched_data.x\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     h_node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgnn_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     h_graph = \u001b[38;5;28mself\u001b[39m.pool(h_node, batched_data.batch)\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_graph_features > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mGNN_node_Virtualnode_With_Features.forward\u001b[39m\u001b[34m(self, batched_data)\u001b[39m\n\u001b[32m     78\u001b[39m h_list[layer] = h_list[layer] + virtualnode_embedding[batch]\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convs[layer].\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33mGINConv\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     82\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convs[layer].\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33mGCNConv\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     83\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.convs[layer](h_list[layer], edge_index, edge_weight=edge_attr \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m# GCNConv can take edge_weight\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/gin_conv.py:90\u001b[39m, in \u001b[36mGINConv.forward\u001b[39m\u001b[34m(self, x, edge_index, size)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     out = out + (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.eps) * x_r\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py:173\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.track_running_stats:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.momentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[32m    175\u001b[39m             exponential_average_factor = \u001b[32m1.0\u001b[39m / \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_batches_tracked)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_accuracy = 0.0\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "if num_checkpoints > 1:\n",
    "    checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "else:\n",
    "    checkpoint_intervals = [num_epochs]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        train_loader,\n",
    "        model, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        device,\n",
    "        save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
    "        checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
    "        current_epoch=epoch,\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc = evaluate(val_loader, model, device, calculate_accuracy=True, criterion=criterion)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save logs for training progress\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "# Plot training progress in current directory\n",
    "plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loader = DataLoader(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:18<00:00, 17.79batch/s]\n",
      "Epoch 51/60, Loss: 0.4312, Train Acc: 0.7606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_1.pth\n",
      "Epoch 51/60, Loss: 0.4312, Train Acc: 0.7606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:15<00:00, 20.63batch/s]\n",
      "Epoch 52/60, Loss: 0.4232, Train Acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_2.pth\n",
      "Epoch 52/60, Loss: 0.4232, Train Acc: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:17<00:00, 18.37batch/s]\n",
      "Epoch 53/60, Loss: 0.4210, Train Acc: 0.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_3.pth\n",
      "Epoch 53/60, Loss: 0.4210, Train Acc: 0.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:17<00:00, 18.16batch/s]\n",
      "Epoch 54/60, Loss: 0.4059, Train Acc: 0.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_4.pth\n",
      "Epoch 54/60, Loss: 0.4059, Train Acc: 0.7782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:17<00:00, 18.29batch/s]\n",
      "Epoch 55/60, Loss: 0.4020, Train Acc: 0.7792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_5.pth\n",
      "Epoch 55/60, Loss: 0.4020, Train Acc: 0.7792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:17<00:00, 18.93batch/s]\n",
      "Epoch 56/60, Loss: 0.4000, Train Acc: 0.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_6.pth\n",
      "Epoch 56/60, Loss: 0.4000, Train Acc: 0.7790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:15<00:00, 20.14batch/s]\n",
      "Epoch 57/60, Loss: 0.3958, Train Acc: 0.7828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_7.pth\n",
      "Epoch 57/60, Loss: 0.3958, Train Acc: 0.7828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:18<00:00, 17.28batch/s]\n",
      "Epoch 58/60, Loss: 0.3912, Train Acc: 0.7822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_8.pth\n",
      "Epoch 58/60, Loss: 0.3912, Train Acc: 0.7822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:18<00:00, 17.79batch/s]\n",
      "Epoch 59/60, Loss: 0.3915, Train Acc: 0.7874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_9.pth\n",
      "Epoch 59/60, Loss: 0.3915, Train Acc: 0.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating training graphs: 100%|██████████| 322/322 [00:16<00:00, 19.49batch/s]\n",
      "Epoch 60/60, Loss: 0.3868, Train Acc: 0.7913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /workspace/hackaton/checkpoints/D/model_D_epoch_10.pth\n",
      "Epoch 60/60, Loss: 0.3868, Train Acc: 0.7913\n",
      "Best model updated and saved at /workspace/hackaton/checkpoints/model_D_best.pth\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "num_checkpoints = 10\n",
    "\n",
    "if num_checkpoints > 1:\n",
    "    checkpoint_intervals = [50 + int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
    "else:\n",
    "    checkpoint_intervals = [num_epochs]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        full_loader,\n",
    "        model, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        device,\n",
    "        save_checkpoints=(50 + epoch + 1 in checkpoint_intervals),\n",
    "        checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
    "        current_epoch=epoch,\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch + 51}/{num_epochs + 50}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Save logs for training progress\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    logging.info(f\"Epoch {epoch + 51}/{num_epochs + 50}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"Best model updated and saved at {checkpoint_path}\")\n",
    "\n",
    "# Plot training progress in current directory\n",
    "plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
    "plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GraphDatasetWithFeatures(args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating eval graphs: 100%|██████████| 71/71 [00:05<00:00, 12.79batch/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /workspace/hackaton/submission/testset_D.csv\n"
     ]
    }
   ],
   "source": [
    "save_predictions(predictions, args.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best Epoch 47/50, Loss: 0.4592, Val loss: 0.5975, Train Acc: 0.7461, Val Acc 0.6941\n",
    "# needs retrain\n",
    "\n",
    "# num_layer=5,\n",
    "# emb_dim=300,\n",
    "# noize 0.3\n",
    "# without training on the full dataset\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/hackaton\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './submission' has been compressed into './submission.gz'\n"
     ]
    }
   ],
   "source": [
    "def gzip_folder(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Compresses an entire folder into a single .tar.gz file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder to compress.\n",
    "        output_file (str): Path to the output .tar.gz file.\n",
    "    \"\"\"\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        tar.add(folder_path, arcname=os.path.basename(folder_path))\n",
    "    print(f\"Folder '{folder_path}' has been compressed into '{output_file}'\")\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./submission\"            # Path to the folder you want to compress\n",
    "output_file = \"./submission.gz\"         # Output .gz file name\n",
    "gzip_folder(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 357374,
     "modelInstanceId": 340402,
     "sourceId": 417260,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
